{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from hydra import compose, initialize\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "with initialize(version_base=None, config_path=\"./config\"):\n",
    "    cfg = compose(config_name=\"properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from omegaconf import DictConfig\n",
    "from openai import RateLimitError\n",
    "from operator import itemgetter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnablePassthrough, ConfigurableField\n",
    "\n",
    "def build_chat_model(cfg: DictConfig):\n",
    "    model = AzureChatOpenAI(\n",
    "        **cfg.llm.openai\n",
    "    ).configurable_alternatives(\n",
    "        ConfigurableField(id=\"llm_type\"),\n",
    "        default_key=\"openai\",\n",
    "        llama=ChatOllama(**cfg.llm.llama)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def format_llm_messages(inputs: dict):\n",
    "    if (human_message:= inputs.get(\"question\")) is None:\n",
    "        raise KeyError(\"`question` parameter not found in during `invoke`.\")\n",
    "    system_message = (\n",
    "        \"You are a helpful AI bot.\" \n",
    "        if not inputs.get(\"system_prompt\") else inputs.get(\"system_prompt\")\n",
    "    )\n",
    "    return ChatPromptTemplate([\n",
    "        (\"system\", system_message), (\"human\", human_message)\n",
    "    ])\n",
    "\n",
    "def format_lmm_messages(inputs: dict):\n",
    "    # set system prompt\n",
    "    system_message = (\n",
    "        \"You are a helpful AI bot.\" \n",
    "        if not inputs.get(\"system_prompt\") else inputs.get(\"system_prompt\")\n",
    "    )\n",
    "    # format prompt\n",
    "    human_messages = [{\"type\": \"text\", \"text\" : inputs[\"question\"]}]\n",
    "    image_urls = inputs[\"images\"]\n",
    "    for image_url in image_urls:\n",
    "        human_messages += [\n",
    "            {\n",
    "                \"type\" : \"image_url\",\n",
    "                \"image_url\" : {\"url\" : image_url}\n",
    "            }     \n",
    "        ]\n",
    "    return [SystemMessage(content=system_message), HumanMessage(content=human_messages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ATTEMPT = 3\n",
    "model = build_chat_model(cfg)\n",
    "\n",
    "llm_chain = (\n",
    "    format_llm_messages\n",
    "    | model.with_retry(\n",
    "        retry_if_exception_type=(RateLimitError,),\n",
    "        stop_after_attempt=MAX_ATTEMPT,\n",
    "        wait_exponential_jitter=True\n",
    "    )\n",
    ")\n",
    "\n",
    "response = llm_chain.with_config(\n",
    "    configurable={\"llm_type\": \"openai\"}\n",
    ").invoke({\"question\": \"What's your name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ATTEMPT = 3\n",
    "model = build_chat_model(cfg)\n",
    "\n",
    "lmm_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"system_prompt\": itemgetter(\"system_prompt\"),\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"images\": itemgetter(\"images\") | RunnableLambda(\n",
    "                lambda x: [f\"data:image/jpeg;base64,{encode_image(_)}\" for _ in x]\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    | format_lmm_messages\n",
    "    | model.with_retry(\n",
    "        retry_if_exception_type=(RateLimitError,),\n",
    "        stop_after_attempt=MAX_ATTEMPT,\n",
    "        wait_exponential_jitter=True\n",
    "    )\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"You are an Optical Character Recognition machine.\n",
    "You will extract all the characters from the image provided by the user, and you will only privide the extracted text in your response.\n",
    "As an OCR machine, You can only respond with the extracted text according to the following intruction.\n",
    "* Do not modify any of the content in the given image.\n",
    "* Skip the preamble in your answer.\n",
    "* Format your answer with structurized information such as markdown or html.\n",
    "* Do not translate any of the content in the given image. Return as it is.\"\"\"\n",
    "\n",
    "response = lmm_chain.with_config(\n",
    "    configurable={\"llm_type\": \"openai\"}\n",
    ").invoke(\n",
    "    {\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"question\": \"이미지에 있는 텍스트를 원본 그대로 추출해줘.\",\n",
    "        \"images\": [\"./data/food_list.png\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "    \n",
    "model = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"llama3.2-vision\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "    \n",
    "]\n",
    "ai_message = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je suis amoureux de la programmation.', response_metadata={'model': 'llama3.2-vision', 'created_at': '2024-11-12T02:34:08.28585434Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 21542337784, 'load_duration': 4831561776, 'prompt_eval_count': 32, 'prompt_eval_duration': 5160000000, 'eval_count': 11, 'eval_duration': 10822000000}, id='run-043099dc-59d7-4f62-9e60-b940f795d7c8-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
