{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you for asking! As an AI language model, I don't have feelings, but I'm here and ready to assist you with any historical questions you may have. How can I help you today?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=StrOutputParser()\n",
    ")\n",
    "chain.run(question=\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize from string\n",
    "template = \"\"\"Tell me a {adjective} joke about {subject}.\"\"\"\n",
    "llm_chain = LLMChain.from_string(llm=llm, template=template)\n",
    "llm_chain.predict(adjective=\"sad\", subject=\"ducks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## __call__ method\n",
    "## By default, it returns both the input and output key values.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "llm_chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "print(llm_chain(inputs={\"adjective\": \"corny\"}))\n",
    "\n",
    "## add return_only_outputs=True\n",
    "## Also, in the case of one input key, you can input the string directly without specifying the input mapping.\n",
    "llm_chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "print(llm_chain(\"corny\", return_only_outputs=True))\n",
    "\n",
    "## run method\n",
    "# IF only one output_keys, you can use run method\n",
    "print(llm_chain.output_keys)\n",
    "print(llm_chain.run({\"adjective\":\"corny\"}))\n",
    "print(\"run: \\n\", llm_chain.run(\"corny\"))\n",
    "\n",
    "## predict method\n",
    "# similar to run, but the input_keys are specified as keyword arguments instead of a python dict\n",
    "# print(\"predict: \\n\", llm_chain.predict(adjective=\"corney\"))\n",
    "# multiple inputs example\n",
    "template = \"\"\"Tell me a {adjective} joke about {subject}.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"subject\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(\"predict: \\n\", llm_chain.predict(adjective=\"sad\", subject=\"ducks\"))\n",
    "\n",
    "## apply method\n",
    "## allows you run the chain against a list of tuples\n",
    "prompt_template = \"What is a good name for a company that makes {product}?\"\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "input_list = [{\"product\": \"socks\"}, {\"product\": \"computer\"}, {\"product\": \"shoes\"}]\n",
    "print(\"apply: \\n\", llm_chain.apply(input_list))\n",
    "\n",
    "## generate method\n",
    "## similar to apply, except it return an LLMResult\n",
    "print(\"generate: \\n\", llm_chain.generate(input_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The colors in a rainbow are:\\n\\n1. Red\\n2. Orange\\n3. Yellow\\n4. Green\\n5. Blue\\n6. Indigo\\n7. Violet']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add output_parser to prompt_template\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"List all the colors in a rainbow\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template, input_variables=[]\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MyCustomChain chain...\u001b[0m\n",
      "Log something about this run\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did the function go to therapy?\\n\\nBecause it had too many unresolved callbacks!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Custom Chain\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "\n",
    "\n",
    "class MyCustomChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = self.llm.generate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\"Log something about this run\")\n",
    "\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = await self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\"Log something about this run\")\n",
    "\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"my_custom_chain\"\n",
    "    \n",
    "from langchain.callbacks.stdout import StdOutCallbackHandler\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "chain = MyCustomChain(\n",
    "    prompt=PromptTemplate.from_template(\"tell us a joke about {topic}\"),\n",
    "    llm=ChatOpenAI(),\n",
    ")\n",
    "\n",
    "chain.run({\"topic\": \"callbacks\"}, callbacks=[StdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The next four colors of a rainbow are green, blue, indigo, and violet.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Adding memory\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.run(\"Answer briefly. What are the first 3 colors of a rainbow?\")\n",
    "# -> The first three colors of a rainbow are red, orange, and yellow.\n",
    "conversation.run(\"And the next 4?\")\n",
    "# -> The next four colors of a rainbow are green, blue, indigo, and violet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import asyncio\n",
    "from typing import Any, Dict, List, Optional\n",
    "from functools import partial\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from langchain.callbacks.manager import CallbackManagerForRetrieverRun, AsyncCallbackManagerForRetrieverRun\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s:%(message)s\",\n",
    "    level=logging.DEBUG,\n",
    "    datefmt=\"%m%d%Y %I:%M:%S %p\"\n",
    ")\n",
    "\n",
    "class IRRetriever(BaseRetriever):\n",
    "    \"\"\"IRRetriever to retrieve documents using IR.\"\"\"\n",
    "    \n",
    "    ir_model: Any\n",
    "    \"\"\"IR vectorizer\"\"\"\n",
    "    topk: int=5\n",
    "    \"\"\"Number of documents to return\"\"\"\n",
    "    search_args: Dict=None\n",
    "    \"\"\"Search arguments of IR\"\"\"\n",
    "    retrieved_docs: List[Document]=None\n",
    "    \"\"\"List of documents retrieved\"\"\"\n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object\"\"\"\n",
    "        \n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    \n",
    "    def check_is_project_exist(self, project_name, where):\n",
    "        projects = self.ir_model.list_projects()[where]\n",
    "        is_exist = False\n",
    "        for project in projects:\n",
    "            name = project.get(\"name\")\n",
    "            if project_name == name:\n",
    "                is_exist = True\n",
    "        return is_exist\n",
    "    \n",
    "    def create_new_project(self, project_name, where, index_args=None):\n",
    "        if index_args is None:\n",
    "            index_args = self.ir_model.get_default_index_args()\n",
    "        \n",
    "        self.ir_model.new_project(\n",
    "            project_name=project_name, where=where, index_args=index_args\n",
    "        )\n",
    "        logging.info(\"New project named `{}` created successfully.\")\n",
    "\n",
    "    def load_project(self, project_name, where):\n",
    "        is_exist = self.check_is_project_exist(project_name, where)\n",
    "        if not is_exist:\n",
    "            raise ValueError(\n",
    "                f\"{project_name} not found in {where}. Please create the project first.\"\n",
    "            )\n",
    "        else:\n",
    "            # load project -> get self.search_args\n",
    "            self.search_args = self.ir_model.load_project(\n",
    "                project_name=project_name, where=where\n",
    "            )\n",
    "            logging.info(\"Load project succesfully.\")\n",
    "    \n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get documents relevant to a query.\n",
    "        Args:\n",
    "            query: String to find relevant documents for\n",
    "            run_manager: The callbacks handler to use\n",
    "        Returns:\n",
    "            List of relevant documents\n",
    "        \"\"\"\n",
    "        if self.search_args is None:\n",
    "            raise Exception(\"No project loaded. Please use `load_project` method to load project in advance.\")\n",
    "        \n",
    "        self.search_args[\"tok_k\"] = self.topk\n",
    "        docs = self.ir_model.search(query, search_args=self.search_args)\n",
    "        \n",
    "        self.retrieved_docs = []\n",
    "        for i, doc in enumerate(docs, start=1):\n",
    "            title, context = doc.pop(\"title\"), doc.pop(\"context\")\n",
    "            page_content = f\"{str(i)}. {title}\\n{context}\"\n",
    "            self.retrieved_docs += [\n",
    "                Document(\n",
    "                    page_content=page_content,\n",
    "                    meta_data=doc,\n",
    "                    type=\"Document\"\n",
    "                )\n",
    "            ]\n",
    "        return self.retrieved_docs\n",
    "    \n",
    "    async def _aget_relevant_documents(\n",
    "        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Asynchronously get documents relevant to a query.\n",
    "        Args:\n",
    "            query: String to find relevant documents for\n",
    "            run_manager: The callbacks handler to use\n",
    "        Returns:\n",
    "            List of relevant documents\n",
    "        \"\"\"\n",
    "        return await asyncio.get_running_loop().run_in_executor(\n",
    "            None, partial(self._get_relevant_documents, run_manager=run_manager), query\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import inspect\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "\n",
    "\n",
    "class IRChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    retriever: BaseRetriever\n",
    "    output_key: str = \"answer\"\n",
    "    return_source_documents: bool = True\n",
    "    \"\"\"Return the retrieved source documents as part of the final result.\"\"\"\n",
    "    response_if_no_docs_found: Optional[str]\n",
    "    \"\"\"If specified, the chain will return a fixed response if no docs \n",
    "    are found for the question. \"\"\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "        allow_population_by_field_name = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\"\"\"\n",
    "        \n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Return the output keys.\"\"\"\n",
    "        _output_keys = [self.output_key]\n",
    "        if self.return_source_documents:\n",
    "            _output_keys = _output_keys + [\"source_documents\"]\n",
    "        return _output_keys\n",
    "    \n",
    "    def _get_docs(\n",
    "        self,\n",
    "        question: str,\n",
    "        inputs: Dict[str, Any],\n",
    "        *,\n",
    "        run_manager: CallbackManagerForChainRun,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get relevant documents.\"\"\"\n",
    "        docs = self.retriever.get_relevant_documents(\n",
    "            question, callbacks=run_manager.get_child()\n",
    "        )\n",
    "        return docs\n",
    "    \n",
    "    async def _aget_docs(\n",
    "        self,\n",
    "        question: str,\n",
    "        inputs: Dict[str, Any],\n",
    "        *,\n",
    "        run_manager: AsyncCallbackManagerForChainRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get relevant documents.\"\"\"\n",
    "        docs = await self.retriever.aget_relevant_documents(\n",
    "            question, callbacks=run_manager.get_child()\n",
    "        )\n",
    "        return docs\n",
    "\n",
    "    def create_document_scratchpad(self, docs: List[Document]) -> str:\n",
    "        document_scratchpad = \"\"\n",
    "        for doc in docs:\n",
    "            document_scratchpad += f\"\"\"{doc.page_content}\\n\\n\"\"\"\n",
    "        return document_scratchpad\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        \n",
    "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
    "        question = inputs[\"question\"]\n",
    "        \n",
    "        accepts_run_manager = (\n",
    "            \"run_manager\" in inspect.signature(self._get_docs).parameters\n",
    "        )\n",
    "        if accepts_run_manager:\n",
    "            docs = self._get_docs(question, inputs, run_manager=_run_manager)\n",
    "        else:\n",
    "            docs = self._get_docs(question, inputs)\n",
    "            \n",
    "        document_scratchpad = self.create_document_scratchpad(docs)\n",
    "        \n",
    "        output: Dict[str, Any] = {}\n",
    "        if self.response_if_no_docs_found is not None and len(docs) == 0:\n",
    "            output[self.output_key] = self.response_if_no_docs_found\n",
    "        else:\n",
    "            new_inputs = inputs.copy()\n",
    "            new_inputs[\"document_scratchpad\"] = document_scratchpad\n",
    "            \n",
    "            prompt_value = self.prompt.format_prompt(**inputs)\n",
    "            answer = self.llm.generate_prompt(\n",
    "                [prompt_value], callbacks=_run_manager.get_child()\n",
    "            )\n",
    "            output[self.output_key] = answer\n",
    "        \n",
    "        if self.return_source_documents:\n",
    "            output[\"source_documents\"] = docs\n",
    "        \n",
    "        if _run_manager:\n",
    "            _run_manager.on_text(\"IRChain finally runs!!!\")\n",
    "\n",
    "        return output\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
    "        question = inputs[\"question\"]\n",
    "        \n",
    "        accepts_run_manager = (\n",
    "            \"run_manager\" in inspect.signature(self._get_docs).parameters\n",
    "        )\n",
    "        if accepts_run_manager:\n",
    "            docs = await self._aget_docs(question, inputs, run_manager=_run_manager)\n",
    "        else:\n",
    "            docs = await self._aget_docs(question, inputs)\n",
    "            \n",
    "        document_scratchpad = self.create_document_scratchpad(docs)\n",
    "        \n",
    "        output: Dict[str, Any] = {}\n",
    "        if self.response_if_no_docs_found is not None and len(docs) == 0:\n",
    "            output[self.output_key] = self.response_if_no_docs_found\n",
    "        else:\n",
    "            new_inputs = inputs.copy()\n",
    "            new_inputs[\"document_scratchpad\"] = document_scratchpad\n",
    "            \n",
    "            prompt_value = self.prompt.format_prompt(**inputs)\n",
    "            answer = await self.llm.agenerate_prompt(\n",
    "                [prompt_value], callbacks=_run_manager.get_child()\n",
    "            )\n",
    "            output[self.output_key] = answer\n",
    "        \n",
    "        if self.return_source_documents:\n",
    "            output[\"source_documents\"] = docs\n",
    "        \n",
    "        if _run_manager:\n",
    "            _run_manager.on_text(\"IRChain finally runs!!!\")\n",
    "\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"IR_Chain\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
