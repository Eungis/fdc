{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27b17e9-a4d7-4aa1-b7b0-dcc56182956f",
   "metadata": {},
   "source": [
    "## User Guide on LangChain custom Retriever and LLMChain\n",
    ":: This guide introduces way to use the IRRetriever and IRChain.\n",
    "\n",
    "- Its purpose is on integrating the IR and LangChain interface.\n",
    "- It wraps the IR model with LangChain BaseRetriever. (see retreivers.py)\n",
    "- It inherits the LangChain LLMChain to make IRChain, which enable the user to seamlessly connect IRRetriever and LLMChain. (see chains.py)\n",
    "\n",
    "\n",
    "[TODO]\n",
    "- Solve the max_token exceeding error.\n",
    "- Integrate the IRRetriever with LangChain RetrievalQA.\n",
    "\n",
    "```\n",
    "- Writer: Eungis\n",
    "- last update: 23.11.10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709b48a-cf2a-4b75-9da6-5670ed354f26",
   "metadata": {},
   "source": [
    "## IRRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "import IR\n",
    "from retrievers import IRRetriever\n",
    "\n",
    "logging = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ir_model = IR()\n",
    "retriever = IRRetriever(ir_model=ir_model, top_k=5)\n",
    "# And it assumed the documents have already been indexed before.\n",
    "# Please load the project that have indexed documents when you use the IRRetriever.\n",
    "retriever.load_project(\"{YOUR_PROJECT}\", \"{WHERE}\")\n",
    "docs = retriever._get_relevant_documents(\"{YOUR_QUESTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5506b9-11ea-435a-91be-864bcf92ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# CONFIG contains the API key such as OPENAI_API_KEY, etc.\n",
    "# Load your api key to use LLM.\n",
    "CONFIG = yaml.load(open(\"../config.yaml\"), Loader=yaml.FullLoader)\n",
    "ANTHROPIC_CONFIG = CONFIG.get(\"anthropic\")\n",
    "OPENAI_CONFIG = CONFIG.get(\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8da9af-e2fc-4697-a7af-79e9710ca867",
   "metadata": {},
   "source": [
    "## Simple usage of IRChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71965479-f8f7-409e-8536-1aeba60412ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chains import IRChain\n",
    "from utils import load_ir_chain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# Prepare your prompt\n",
    "system_template = \"\"\"You are an assistant chatbot. Answer the question as best as you can.\"\"\"\n",
    "\n",
    "human_template = \"\"\"Below are documents provided.\n",
    "{context}\n",
    "\n",
    "-------------\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=system_template),\n",
    "        HumanMessagePromptTemplate.from_template(human_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize llm and chain\n",
    "chat_model = AzureChatOpenAI(**OPENAI_CONFIG)\n",
    "chain = load_ir_chain(\n",
    "    llm=chat_model,\n",
    "    retriever=retriever,\n",
    "    prompt=prompt,\n",
    "    input_key=\"question\",\n",
    "    document_variable_name=\"context\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b398b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain(\"{YOUR_QUESTION}\")\n",
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f982b1-6008-445e-b511-90b6143e54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also add memory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Prepare your prompt\n",
    "system_template = \"\"\"You are an assistant chatbot. Answer the question as best as you can.\"\"\"\n",
    "\n",
    "human_template = \"\"\"Below are documents provided.\n",
    "{context}\n",
    "\n",
    "-------------\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "Human: {question}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=system_template),\n",
    "        HumanMessagePromptTemplate.from_template(human_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add memory\n",
    "# **Attention**\n",
    "# 1. As the prompt above contains multiple input_variables(context, question), you must specify what your input_key and output_key are.\n",
    "# 2. The memory_key and the input_variable of the memory in the prompt should be same.\n",
    "# 3. The input_key and the input_variable of the user input in the prompt should be same.\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3,\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "chain = load_ir_chain(\n",
    "    llm=chat_model,\n",
    "    retriever=retriever,\n",
    "    prompt=prompt,\n",
    "    input_key=\"question\",\n",
    "    document_variable_name=\"context\",\n",
    "    memory=True,\n",
    "    memory_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c05bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain(\"{YOUR_QUESTION}\")\n",
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e143832",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain(\"{YOUR_QUESTION}\")\n",
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.memory.buffer_as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf13fe-153c-4929-9eac-3caf9d7e0b20",
   "metadata": {},
   "source": [
    "## IRChain: without memory\n",
    "- Below are the specific guide about how to use IRChain, without attaching the memory to the chain.\n",
    "\n",
    "    - 1. customize document_prompt\n",
    "        - see `format_document` function in langchain.schema to know how each document is formatted.\n",
    "        - below is the example of document_prompt.\n",
    "        - all the documents will be joined according to the `document_separator` parameter in the IRChain.\n",
    "    - 2. add callbacks\n",
    "        - you can add callbacks to the IRChain as well as the llm.\n",
    "        - you can also add callbacks in running the chain.\n",
    "        - pertaining to the detail functionality of callbacks, please refer to the langchain docs. IRChain has exactly the same mechanism as langchain.chains.LLMChain.\n",
    "    - 3. add output_parser\n",
    "    - 4. dynamic call function of IRChain\n",
    "        - _call_, predict, run, generate, apply, etc.\n",
    "        - all the functions are based on `generate`, but with some differences in output format.\n",
    "        - see the langchain documentation to know how the LLMChain works. IRChain has exactly the same mechanism as it.\n",
    "    - 5. support asynchronization function.\n",
    "        - it is useful when you combine the callbacks attached to the `on_new_token` with Frontend-side streaming functionality.\n",
    "        - it is also useful when you want to speed up the test.\n",
    "            - suppose you have 1000 questions to test.\n",
    "            - you can asynchronously run the chain to accerate the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1012e1c-6b07-4ffa-ae21-fddea6a13688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chains import IRChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# This controls how each document will be formatted. Specifically,\n",
    "# it will be passed to `format_document` - see that function for more\n",
    "# details.\n",
    "document_prompt = PromptTemplate.from_template(\"Title {title}\\n{page_content}\")\n",
    "\n",
    "# Prepare your prompt\n",
    "system_template = \"\"\"You are an assistant chatbot. Answer the question as best as you can.\"\"\"\n",
    "\n",
    "human_template = \"\"\"Below are documents provided.\n",
    "{context}\n",
    "\n",
    "-------------\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=system_template),\n",
    "        HumanMessagePromptTemplate.from_template(human_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize llm and IRChain\n",
    "\n",
    "# You can add streaming callbacks to llm model\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# OPENAI_CONFIG.update({\n",
    "#     \"streaming\": True,\n",
    "#     \"callback_manager\": CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# })\n",
    "\n",
    "chat_model = AzureChatOpenAI(**OPENAI_CONFIG)\n",
    "\n",
    "# As IRChain inherits from LLMChain, it has the same interface as it.\n",
    "# That means, you can add callbacks, output_parser, or any other modules\n",
    "# provided through langchain.\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "chain = IRChain(\n",
    "    prompt=prompt,\n",
    "    document_prompt=document_prompt,\n",
    "    llm=chat_model,\n",
    "    retriever=retriever,\n",
    "    document_variable_name=\"context\",\n",
    "    document_separator=\"\\n\\n\",\n",
    "    output_key=\"answer\",\n",
    "    return_final_only=False,\n",
    "    output_parser=StrOutputParser()\n",
    "    # callbacks=[StdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import BaseCallbackHandler\n",
    "from langchain.schema import Document\n",
    "from typing import Sequence, Optional, Any\n",
    "from uuid import UUID\n",
    "\n",
    "\n",
    "# A benefit achieved from the langchain interface is that we can add callback handler also to the Retriever.\n",
    "# Without hard-coding the retrieved documents, such as filtering based on any condition,\n",
    "# we can simpy use callback handler on_retriever_start, on_retriever_end, etc to perform anything we want.\n",
    "# Below is just the example to print out the number of documents we retrieve through IR.\n",
    "class DocumentCallbackHandler(BaseCallbackHandler):\n",
    "    def on_retriever_end(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        print(f\"on_retriever_end() called with {len(documents)} documents\")\n",
    "\n",
    "\n",
    "# You can see that, if related documents provided,\n",
    "# the document_prompt formatted (=context) is set as \"\".\n",
    "# You can change it with setting `document_prompt_if_no_docs_found` parameter in the chain\n",
    "answer = chain(\"안녕\", callbacks=[DocumentCallbackHandler()])\n",
    "print(f\"__call__ output: {type(answer)}\\noutput: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of __call__ method, you can also use\n",
    "# predict, apply, run (only when len(output_keys) == 1) just as LLMChain.\n",
    "print(f\"length of output_keys: {len(chain.output_keys)}\")\n",
    "answer = chain.run({\"question\": \"{YOUR_QUESTION}\"}, callbacks=[DocumentCallbackHandler()])\n",
    "print(f\"run output: {type(answer)}\\noutput: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb909ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.predict(question=\"{YOUR_QUESTION}\", callbacks=[DocumentCallbackHandler()])\n",
    "print(f\"predict output: {type(answer)}\\noutput: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.generate([{\"question\": \"{YOUR_QUESTION}\"}, {\"question\": \"{YOUR_QUESTION}\"}])\n",
    "print(f\"generate output: {type(answer)}, \\noutput: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.apply([{\"question\": \"{YOUR_QUESTION}\"}, {\"question\": \"{YOUR_QUESTION}\"}])\n",
    "print(f\"apply output: {type(answer)}, \\noutput[0] contains: {answer[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It also supports async function.\n",
    "answer = await chain.apredict(question=\"{YOUR_QUESTION}\")\n",
    "print(f\"apredict output: {type(answer)}\\noutput: {answer}\")\n",
    "\n",
    "answer = await chain.agenerate(\n",
    "    [{\"question\": \"{YOUR_QUESTION}\"}, {\"question\": \"{YOUR_QUESTION}\"}]\n",
    ")\n",
    "print(f\"agenerate output: {type(answer)}, \\noutput: {answer}\")\n",
    "\n",
    "answer = await chain.aapply(\n",
    "    [{\"question\": \"{YOUR_QUESTION}\"}, {\"question\": \"{YOUR_QUESTION}\"}]\n",
    ")\n",
    "print(f\"aapply output: {type(answer)}, \\noutput[0] contains: {answer[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41c618-d09a-4424-91dc-aef0133b818d",
   "metadata": {},
   "source": [
    "## IRChain: with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f44b9633-6f16-46f5-ab54-cdeb6439154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chains import IRChain\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.prompts import PromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# This controls how each document will be formatted. Specifically,\n",
    "# it will be passed to `format_document` - see that function for more\n",
    "# details.\n",
    "document_prompt = PromptTemplate.from_template(\"Title {title}\\n{page_content}\")\n",
    "\n",
    "# Prepare your prompt\n",
    "system_template = \"\"\"You are an assistant chatbot. Answer the question as best as you can.\"\"\"\n",
    "\n",
    "human_template = \"\"\"Below are documents provided.\n",
    "{context}\n",
    "\n",
    "-------------\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "Human: {question}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=system_template),\n",
    "        HumanMessagePromptTemplate.from_template(human_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize llm and IRChain\n",
    "# You can add streaming callbacks to llm model\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat_model = ChatAnthropic(**ANTHROPIC_CONFIG)\n",
    "\n",
    "# As IRChain inherits from LLMChain, it has the same interface as it.\n",
    "# That means, you can add callbacks, output_parser, or any other modules\n",
    "# provided through langchain.\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# Add memory\n",
    "# **Attention**\n",
    "# 1. As the prompt above contains multiple input_variables(context, question), you must specify what your input_key and output_key are.\n",
    "# 2. The memory_key and the input_variable of the memory in the prompt should be same.\n",
    "# 3. The input_key and the input_variable of the user input in the prompt should be same.\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3,\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "chain = IRChain(\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    document_prompt=document_prompt,\n",
    "    llm=chat_model,\n",
    "    retriever=retriever,\n",
    "    document_variable_name=\"context\",\n",
    "    document_separator=\"\\n\\n\",\n",
    "    output_key=\"answer\",\n",
    "    return_final_only=False,\n",
    "    output_parser=StrOutputParser(),\n",
    "    # callbacks=[StdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8511ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain(\"{YOUR_QUESTION}\", callbacks=[DocumentCallbackHandler()])\n",
    "print(f\"__call__ output: {type(answer)}\\noutput contains: {answer.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory\n",
    "print(chain.memory.buffer_as_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac2b77-7a9b-4f65-9d7e-4a623ab887bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import timeit\n",
    "\n",
    "\n",
    "# You can use this decorator to count time spent on function running.\n",
    "@timeit\n",
    "def run_chain(chain, question):\n",
    "    return chain.predict(question=question)\n",
    "\n",
    "\n",
    "run_chain(chain, \"{YOUR_QUESTION}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
