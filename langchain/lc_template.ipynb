{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage of Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='VibrantSock Co.')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# load env variables\n",
    "load_dotenv()\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "chat_model = ChatOpenAI(temperature=0)\n",
    "\n",
    "## predict\n",
    "llm.predict(\"hi!\")\n",
    "chat_model.predict(\"hi!\")\n",
    "\n",
    "## predict_messages\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "llm.predict_messages(messages)\n",
    "chat_model.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"What is a good name for a company that makes {product}?\"\n",
    ")\n",
    "prompt.format(product=\"colorful socks\")\n",
    "\n",
    "# It sames as:\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant that translates English to French.'), HumanMessage(content='I love programming.')]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful assistant that translates English to French.\n",
      "Human: I love programming.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"J'adore la programmation.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    ")\n",
    "human_template = \"{text}\"\n",
    "\n",
    "# Use one of 'human', 'user', 'ai', 'assistant', or 'system'\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", template), (\"human\", human_template)]\n",
    ")\n",
    "chat_prompt.format_messages(\n",
    "    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n",
    ")\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    ")\n",
    "human_template = \"{text}\"\n",
    "\n",
    "# Use one of 'human', 'user', 'ai', 'assistant', or 'system'\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", template), (\"human\", human_template)]\n",
    ")\n",
    "output = chat_prompt.format(\n",
    "    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n",
    ")\n",
    "# or alternatively\n",
    "output_as_ChatPromptValue = chat_prompt.format_prompt(\n",
    "    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n",
    ")\n",
    "# format_prompt -> ChatPromptValue\n",
    "assert output == output_as_ChatPromptValue.to_string()\n",
    "\n",
    "output_as_messages = chat_prompt.format_prompt(\n",
    "    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n",
    ").to_messages()\n",
    "print(output_as_messages)\n",
    "\n",
    "\n",
    "# see how the llm get inputs\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=chat_model, prompt=chat_prompt, verbose=True)\n",
    "chain.run(\n",
    "    input_language=\"English\", output_language=\"French\", text=\"I love programming.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chat_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep dive into PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a joke.\n",
      "---------------------\n",
      "Tell me a funny joke about chickens.\n",
      "Parameter:\n",
      "      {'template_format': 'f-string'}\n",
      "      ['adjective', 'content']\n",
      "      Tell me a {adjective} joke about {content}.\n",
      "      f-string\n",
      "      False\n",
      "      prompt\n",
      "---------------------\n",
      "def sample_function(a, b):\n",
      "    return a%b\n",
      "\n",
      "Given the function name and source code, generate an English language explanation of the function.\n",
      "Function Name: get_source_code\n",
      "Source Code:\n",
      "def get_source_code(function_name):\n",
      "    return inspect.getsource(function_name)\n",
      "\n",
      "Explanation:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, BasePromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke.\")\n",
    "print(prompt_template.format())\n",
    "print(\"---------------------\")\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "print(prompt_template.format(adjective=\"funny\", content=\"chickens\"))\n",
    "print(\n",
    "    f\"\"\"Parameter:\n",
    "      {prompt_template.lc_attributes}\n",
    "      {prompt_template.input_variables}\n",
    "      {prompt_template.template}\n",
    "      {prompt_template.template_format}\n",
    "      {prompt_template.validate_template}\n",
    "      {prompt_template._prompt_type}\"\"\"\n",
    ")\n",
    "\n",
    "print(\"---------------------\")\n",
    "## Specify input_variables explicitly.\n",
    "\n",
    "# invalid_prompt = PromptTemplate(\n",
    "#     input_variables=[\"adjective\"],\n",
    "#     template=\"Tell me a {adjective} joke about {content}.\"\n",
    "# ) -> Validation Error\n",
    "## You can disable the validation by adding:\n",
    "## PromptTemplate(template=template, input_variables=[\"adjective\"], validate_template=False)\n",
    "\n",
    "\n",
    "## Custom PromptTemplate\n",
    "## !pip3 install pydantic==1.10.12\n",
    "## About the issue raised: https://github.com/langchain-ai/langchain/issues/9702\n",
    "\n",
    "# Two requirements:\n",
    "# 1. input_variables that the custom PromptTemplate expects\n",
    "# 2. format() method that takes in the keyword arguments corresponding to the expected input_variables\n",
    "# and returns the formatted prompt.\n",
    "\n",
    "# Two distinct prompt templates:\n",
    "# 1. StringPromptTemplate\n",
    "# 2. ChatPromptTemplate\n",
    "\n",
    "import inspect\n",
    "\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    return inspect.getsource(function_name)\n",
    "\n",
    "\n",
    "def sample_function(a, b):\n",
    "    return a % b\n",
    "\n",
    "\n",
    "print(get_source_code(sample_function))\n",
    "\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "PROMPT = \"\"\"\\\n",
    "Given the function name and source code, generate an English language explanation of the function.\n",
    "Function Name: {function_name}\n",
    "Source Code:\n",
    "{source_code}\n",
    "Explanation:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    \"\"\"A custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.\"\"\"\n",
    "\n",
    "    input_variables: list = Field(\n",
    "        ..., description=\"The input variables for the prompt template\"\n",
    "    )\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\"Validate that the input variables are correct.\"\"\"\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the source code of the function\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # Generate the prompt to be sent to the language model\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__, source_code=source_code\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\"\n",
    "\n",
    "\n",
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "prompt = fn_explainer.format(function_name=get_source_code)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I absolutely love indulging in delicious treats!')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# ChatPromptTemplate accepts a variety of message representations.\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "# Instead of 2-tuple representation, pass an instance of MessagePromptTemplate or BaseMessage.\n",
    "from langchain.prompts import HumanMessagePromptTemplate  # MessagePromptTemplate\n",
    "from langchain.schema.messages import SystemMessage  # BaseMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm(chat_template.format_messages(text=\"i dont like eating tasty things.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PromptTemplate` & `ChatPromptTemplate`\n",
    "- Implement Runnable interface\n",
    "- Runnable interface: the basic block of the LangChain Expression Language (LCEL)\n",
    "- it supports invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\n",
    "\n",
    "`PromptTemplate` accepts a dictionary (of the prompt variables) and returns a `StringPromptValue`. A `ChatPromptTemplate` accepts a dictionary and returns a `ChatPromptValue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about chickens.\n",
      "[HumanMessage(content='Tell me a funny joke about chickens.')]\n",
      "{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['text'], 'messages': [SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]}}\n",
      "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content='i dont like eating tasty things.')]\n",
      "System: You are a helpful assistant that re-writes the user's text to sound more upbeat.\n",
      "Human: i dont like eating tasty things.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt_val = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
    "# StringPromptValue(text='Tell me a funny joke about chickens.')\n",
    "print(prompt_val.to_string())\n",
    "print(prompt_val.to_messages())\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "print(chat_template.to_json())\n",
    "\n",
    "chat_val = chat_template.invoke({\"text\": \"i dont like eating tasty things.\"})\n",
    "# ChatPromptValue(messages=[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content='i dont like eating tasty things.')])\n",
    "print(chat_val.to_messages())\n",
    "print(chat_val.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='May the force be with you' role='Jedi'\n",
      "Jedi: May the force be with you\n",
      "Human: Hi\n",
      "------------\n",
      "Human: What is the best way to learn programming?\n",
      "AI: 1. Choose a programming language: Decide on a programming language that you want to learn.\n",
      "\n",
      "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\n",
      "\n",
      "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\n",
      "Human: Summarize our conversation so far in 10 words.\n"
     ]
    }
   ],
   "source": [
    "# MessagePromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# However, in cases where the chat model supports taking chat message with arbitrary role,\n",
    "# you can use ChatMessagePromptTemplate, which allows user to specify the role name.\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "prompt = \"May the {subject} be with you\"\n",
    "\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(\n",
    "    role=\"Jedi\", template=prompt\n",
    ")\n",
    "chat_message = chat_message_prompt.format(subject=\"force\")\n",
    "print(chat_message)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [chat_message, HumanMessagePromptTemplate.from_template(\"{text}\")]\n",
    ")\n",
    "print(chat_prompt.format(text=\"Hi\"))\n",
    "print(\"------------\")\n",
    "\n",
    "# LangChain also provides MessagesPlaceholder,\n",
    "# which gives you full control of what messages to be rendered during formatting.\n",
    "# This can be useful when you are uncertain of what role you should be using for your message prompt templates or when you wish to insert a list of messages during formatting.\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(variable_name=\"conversation\"), human_message_template]\n",
    ")\n",
    "\n",
    "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"\"\"\\\n",
    "1. Choose a programming language: Decide on a programming language that you want to learn.\n",
    "\n",
    "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\n",
    "\n",
    "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    chat_prompt.format_prompt(\n",
    "        conversation=[human_message, ai_message], word_count=\"10\"\n",
    "    ).to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo BAR\n",
      "foo BAR\n",
      "Tell me a funny joke about the day 10/28/2023 23:09:21\n",
      "Tell me a funny joke about the day 10/28/2023 23:09:21\n"
     ]
    }
   ],
   "source": [
    "## Partial prompt template\n",
    "\n",
    "# Partial formatting with string values.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{foo} {bar}\", input_variables=[\"bar\"], partial_variables={\"foo\": \"foo\"}\n",
    ")\n",
    "print(prompt.format(bar=\"BAR\"))\n",
    "# OR\n",
    "prompt = PromptTemplate(template=\"{foo} {bar}\", input_variables=[\"bar\", \"foo\"])\n",
    "partial_prompt = prompt.partial(foo=\"foo\")\n",
    "print(partial_prompt.format(bar=\"BAR\"))\n",
    "\n",
    "# Partial formatting with functions that return string values.\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def _get_datetime():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\", \"date\"],\n",
    ")\n",
    "partial_prompt = prompt.partial(date=_get_datetime)\n",
    "print(partial_prompt.format(adjective=\"funny\"))\n",
    "# OR\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\"],\n",
    "    partial_variables={\"date\": _get_datetime},\n",
    ")\n",
    "print(prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example_a', 'person', 'example_q', 'input']\n",
      "You are impersonating Elon Musk.\n",
      "\n",
      "Here's an example of an interaction:\n",
      "\n",
      "Q: What's your favorite car?\n",
      "A: Tesla\n",
      "\n",
      "Now, do this for real!\n",
      "\n",
      "Q: What's your favorite social media site?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PipelinePromptTemplate\n",
    "\n",
    "## PipelinePromptTemplate\n",
    "\n",
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "\n",
    "example_template = \"\"\"Here's an example of an interaction:\n",
    "\n",
    "Q: {example_q}\n",
    "A: {example_a}\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "start_template = \"\"\"Now, do this for real!\n",
    "\n",
    "Q: {input}\n",
    "A:\"\"\"\n",
    "start_prompt = PromptTemplate.from_template(start_template)\n",
    "\n",
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt),\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt, pipeline_prompts=input_prompts\n",
    ")\n",
    "print(pipeline_prompt.input_variables)\n",
    "print(\n",
    "    pipeline_prompt.format(\n",
    "        person=\"Elon Musk\",\n",
    "        example_q=\"What's your favorite car?\",\n",
    "        example_a=\"Tesla\",\n",
    "        input=\"What's your favorite social media site?\",\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
