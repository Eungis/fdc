{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3709b48a-cf2a-4b75-9da6-5670ed354f26",
   "metadata": {},
   "source": [
    "## IRRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04419613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "from retrievers import IRRetriever\n",
    "\n",
    "logging = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "retriever = IRRetriever(ir_model=\"<YOUR_MODEL>\", top_k=5)\n",
    "docs = retriever._get_relevant_documents(\"<YOUR_QUESTION>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf13fe-153c-4929-9eac-3caf9d7e0b20",
   "metadata": {},
   "source": [
    "## IRChain: without memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1012e1c-6b07-4ffa-ae21-fddea6a13688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chains import IRChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# This controls how each document will be formatted. Specifically,\n",
    "# it will be passed to `format_document` - see that function for more\n",
    "# details.\n",
    "document_prompt = PromptTemplate.from_template(\n",
    "    \"Title {title}\\n{page_content}\"\n",
    ")\n",
    "\n",
    "system_template = \"\"\"You are an assistant chatbot. Answer the best as you can.\"\"\"\n",
    "\n",
    "human_template = \"\"\"Below are documents provided.\n",
    "{context}\n",
    "\n",
    "-------------\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=system_template),\n",
    "    HumanMessagePromptTemplate.from_template(human_template)\n",
    "])  \n",
    "\n",
    "# Initialize llm and KeylookChain\n",
    "\n",
    "# You can add streaming callbacks to llm model\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key = \"<YOUR_KEY>\",\n",
    "    # streaming=True,\n",
    "    # callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "\n",
    "# As IRChain inherits from LLMChain, it has the same interface as it.\n",
    "# That means, you can add callbacks, output_parser, or any other modules\n",
    "# provided through langchain.\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "chain = IRChain(\n",
    "    prompt = prompt,\n",
    "    document_prompt = document_prompt,\n",
    "    llm = chat_model,\n",
    "    retriever = retriever,\n",
    "    document_variable_name = \"context\",\n",
    "    document_separator = \"\\n\\n\",\n",
    "    output_key = \"answer\",\n",
    "    return_source_documents = True,\n",
    "    return_final_only = False,\n",
    "    output_parser=StrOutputParser()\n",
    "    # callbacks=[StdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c82df25-001a-4965-950c-05cbde5afa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_retriever_end() called with 5 documents\n",
      "__call__ output: <class 'dict'>\n",
      "output contains: dict_keys(['question', 'context', 'answer', 'full_generation', 'source_documents'])\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import BaseCallbackHandler\n",
    "from langchain.schema import Document\n",
    "from typing import Sequence, Optional, Any\n",
    "from uuid import UUID\n",
    "\n",
    "# A benefit achieved from the langchain interface is that we can add callback handler also to the Retriever. \n",
    "# Without hard-coding the retrieved documents, such as filtering based on any condition,\n",
    "# we can simpy use callback handler on_retriever_start, on_retriever_end, etc to perform anything we want.\n",
    "# Below is just the example to print out the number of documents we retrieve through keylook.\n",
    "class DocumentCallbackHandler(BaseCallbackHandler):\n",
    "    def on_retriever_end(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        parent_run_id: Optional[UUID]=None,\n",
    "        **kwargs: Any\n",
    "    ):\n",
    "        print(f\"on_retriever_end() called with {len(documents)} documents\")\n",
    "        \n",
    "answer = chain(\"<YOUR_QUESTION>\", callbacks=[DocumentCallbackHandler()])\n",
    "print(f\"__call__ output: {type(answer)}\\noutput contains: {answer.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of __call__ method, you can also use\n",
    "# predict, apply, run (only when len(output_keys) == 1) just as LLMChain.\n",
    "print(f\"length of output_keys: {len(chain.output_keys)}\")\n",
    "\n",
    "answer = chain.predict(question=\"<YOUR_QUESTION>\", callbacks=[DocumentCallbackHandler()])\n",
    "print(f\"predict output: {type(answer)}\\noutput: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a09c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.generate(\n",
    "    [\n",
    "        {\"question\":\"<YOUR_QUESTION_1>\"},\n",
    "        {\"question\":\"<YOUR_QUESTION_2>\"}\n",
    "    ]\n",
    ")\n",
    "print(f\"generate output: {type(answer)}, \\noutput: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeed876",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.apply(\n",
    "    [\n",
    "        {\"question\":\"<YOUR_QUESTION_1>\"},\n",
    "        {\"question\":\"<YOUR_QUESTION_2>\"}\n",
    "    ]\n",
    ")\n",
    "print(f\"apply output: {type(answer)}, \\noutput[0] contains: {answer[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173dd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It also supports async function.\n",
    "answer = await chain.apredict(question=\"<YOUR_QUESTION>\")\n",
    "print(f\"apredict output: {type(answer)}\\noutput: {answer}\")\n",
    "\n",
    "answer = await chain.agenerate(\n",
    "    [\n",
    "        {\"question\":\"<YOUR_QUESTION_1>\"},\n",
    "        {\"question\":\"<YOUR_QUESTION_2>\"}\n",
    "    ]\n",
    ")\n",
    "print(f\"agenerate output: {type(answer)}, \\noutput: {answer}\")\n",
    "\n",
    "answer = await chain.aapply(\n",
    "    [\n",
    "        {\"question\":\"<YOUR_QUESTION_1>\"},\n",
    "        {\"question\":\"<YOUR_QUESTION_2>\"}\n",
    "    ]\n",
    ")\n",
    "print(f\"aapply output: {type(answer)}, \\noutput[0] contains: {answer[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41c618-d09a-4424-91dc-aef0133b818d",
   "metadata": {},
   "source": [
    "## IRChain: with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f44b9633-6f16-46f5-ab54-cdeb6439154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chains import KeylookChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# This controls how each document will be formatted. Specifically,\n",
    "# it will be passed to `format_document` - see that function for more\n",
    "# details.\n",
    "document_prompt = PromptTemplate.from_template(\n",
    "    \"Title {title}\\n{page_content}\"\n",
    ")\n",
    "\n",
    "system_template = \"\"\"You are an assistant chatbot. Answer the best as you can.\"\"\"\n",
    "\n",
    "human_template = \"\"\"Below are documents provided.\n",
    "{context}\n",
    "\n",
    "-------------\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "Human: {question}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=system_template),\n",
    "    HumanMessagePromptTemplate.from_template(human_template)\n",
    "])\n",
    "\n",
    "# Initialize llm and KeylookChain\n",
    "# You can add streaming callbacks to llm model\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key = \"<YOUR_KEY>\",\n",
    "    # streaming=True,\n",
    "    # callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "\n",
    "# As KeylookChain inherits from LLMChain, it has the same interface as it.\n",
    "# That means, you can add callbacks, output_parser, or any other modules\n",
    "# provided through langchain.\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3,\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "chain = KeylookChain(\n",
    "    memory = memory,\n",
    "    prompt = prompt,\n",
    "    document_prompt = document_prompt,\n",
    "    llm = chat_model,\n",
    "    retriever = retriever,\n",
    "    document_variable_name = \"context\",\n",
    "    document_separator = \"\\n\\n\",\n",
    "    output_key = \"answer\",\n",
    "    return_source_documents = True,\n",
    "    return_final_only = False,\n",
    "    output_parser=StrOutputParser(),\n",
    "    # callbacks=[StdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e835c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain(\"<YOUR_QUESTION>\", callbacks=[DocumentCallbackHandler()])\n",
    "print(f\"__call__ output: {type(answer)}\\noutput contains: {answer.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc68084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of __call__ method, you can also use\n",
    "# predict, apply, run (only when len(output_keys) == 1) just as LLMChain.\n",
    "print(f\"length of output_keys: {len(chain.output_keys)}\")\n",
    "\n",
    "answer = chain.predict(question=\"<YOUR_QUESTION>\", callbacks=[DocumentCallbackHandler()])\n",
    "print(f\"predict output: {type(answer)}\\noutput: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.generate(\n",
    "    [\n",
    "        {\"question\":\"<YOUR_QUESTION_1>\"},\n",
    "        {\"question\":\"<YOUR_QUESTION_2>\"}\n",
    "    ]\n",
    ")\n",
    "print(f\"generate output: {type(answer)}, \\noutput: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.apply(\n",
    "    [\n",
    "        {\"question\":\"<YOUR_QUESTION_1>\"},\n",
    "        {\"question\":\"<YOUR_QUESTION_2>\"}\n",
    "    ]\n",
    ")\n",
    "print(f\"apply output: {type(answer)}, \\noutput[0] contains: {answer[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62159a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It also supports async function.\n",
    "answer = await chain.apredict(question=\"<YOUR_QUESTION>\")\n",
    "print(f\"apredict output: {type(answer)}\\noutput: {answer}\")\n",
    "\n",
    "answer = await chain.agenerate(\n",
    "    [\n",
    "        {\"question\":\"<YOUR_QUESTION_1>\"},\n",
    "        {\"question\":\"<YOUR_QUESTION_2>\"}\n",
    "    ]\n",
    ")\n",
    "print(f\"agenerate output: {type(answer)}, \\noutput: {answer}\")\n",
    "\n",
    "answer = await chain.aapply(\n",
    "    [\n",
    "        {\"question\":\"<YOUR_QUESTION_1>\"},\n",
    "        {\"question\":\"<YOUR_QUESTION_2>\"}\n",
    "    ]\n",
    ")\n",
    "print(f\"aapply output: {type(answer)}, \\noutput[0] contains: {answer[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
