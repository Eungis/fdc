{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs\n",
    "\n",
    "LLMs implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support `invoke`, `ainvoke`, `stream`, `astream`, `batch`, `abatch`, `astream_log` calls.\n",
    "\n",
    "LLMs accept strings as inputs, or objects which can be coerced to string prompts, including List[BaseMessage] and PromptValue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. The Phillips Curve: This theory suggests that there is an inverse relationship between unemployment and inflation. As unemployment decreases, inflation increases, and vice versa.\n",
      "\n",
      "2. The NAIRU Theory: This theory suggests that there is a non-accelerating inflation rate of unemployment (NAIRU). This means that when unemployment is below the NAIRU, inflation will increase, and when unemployment is above the NAIRU, inflation will decrease.\n",
      "\n",
      "3. The Cost-Push Theory: This theory suggests that when the cost of production increases, it leads to higher prices and inflation. This can be caused by an increase in wages due to high unemployment, which can lead to higher prices and inflation.\n",
      "\n",
      "4. The Demand-Pull Theory: This theory suggests that when there is an increase in demand for goods and services, it leads to higher prices and inflation. This can be caused by an increase in consumer spending due to low unemployment, which can lead to higher prices and inflation.\n",
      "-----------------\n",
      "\n",
      "\n",
      "1. The Phillips Curve: This theory suggests that there is an inverse relationship between unemployment and inflation. As unemployment decreases, inflation increases, and vice versa.\n",
      "\n",
      "2. The NAIRU Theory: This theory suggests that there is a non-accelerating inflation rate of unemployment (NAIRU). This means that when unemployment is below the NAIRU, inflation will increase, and when unemployment is above the NAIRU, inflation will decrease.\n",
      "\n",
      "3. The Cost-Push Theory: This theory suggests that when the cost of production increases, it leads to higher prices and inflation. This can be caused by an increase in wages due to high unemployment, which can lead to higher prices and inflation.\n",
      "\n",
      "4. The Demand-Pull Theory: This theory suggests that when there is an increase in demand for goods and services, it leads to higher prices and inflation. This can be caused by an increase in consumer spending due to low unemployment, which can lead to higher prices and inflation.-----------------\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "## invoke\n",
    "res = llm.invoke(\"What are some theories about the relationship between unemployment and inflation?\")\n",
    "print(res)\n",
    "print(\"-----------------\")\n",
    "\n",
    "## stream\n",
    "res = llm.stream(\"What are some theories about the relationship between unemployment and inflation?\")\n",
    "# type(res) = Generator\n",
    "for chunk in res:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print(\"-----------------\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\n1. The Phillips Curve: This theory suggests that there is an inverse relationship between unemployment and inflation. As unemployment decreases, inflation increases, and vice versa.\\n\\n2. The NAIRU Theory: This theory suggests that there is a non-accelerating inflation rate of unemployment (NAIRU). This means that when unemployment is below the NAIRU, inflation will increase, and when unemployment is above the NAIRU, inflation will decrease.\\n\\n3. The Cost-Push Theory: This theory suggests that when the cost of production increases, it leads to higher prices and inflation. This can be caused by an increase in wages due to high unemployment, which can lead to higher prices and inflation.\\n\\n4. The Demand-Pull Theory: This theory suggests that when there is an increase in demand for goods and services, it leads to higher prices and inflation. This can be caused by an increase in consumer spending due to low unemployment, which can lead to higher prices and inflation.']\n",
      "\n",
      "\n",
      "1. The Phillips Curve: This theory suggests that there is an inverse relationship between unemployment and inflation. As unemployment decreases, inflation increases, and vice versa.\n",
      "\n",
      "2. The NAIRU Theory: This theory suggests that there is a non-accelerating inflation rate of unemployment (NAIRU). This means that when unemployment is below the NAIRU, inflation will increase, and when unemployment is above the NAIRU, inflation will decrease.\n",
      "\n",
      "3. The Cost-Push Theory: This theory suggests that when the cost of production increases, it leads to higher prices and inflation. This can be caused by an increase in wages due to high unemployment, which can lead to higher prices and inflation.\n",
      "\n",
      "4. The Demand-Pull Theory: This theory suggests that when there is an increase in demand for goods and services, it leads to higher prices and inflation. This can be caused by an increase in consumer spending due to low unemployment, which can lead to higher prices and inflation."
     ]
    }
   ],
   "source": [
    "## batch\n",
    "res = llm.batch([\"What are some theories about the relationship between unemployment and inflation?\"])\n",
    "print(res)\n",
    "\n",
    "## ainvoke\n",
    "await llm.ainvoke(\"What are some theories about the relationship between unemployment and inflation?\")\n",
    "\n",
    "## astream\n",
    "async for chunk in llm.astream(\"What are some theories about the relationship between unemployment and inflation?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "## abatch\n",
    "await llm.abatch([\"What are some theories about the relationship between unemployment and inflation?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## astram_log\n",
    "# async for chunk in llm.astream_log(\"What are some theories about the relationship between unemployment and inflation?\"):\n",
    "#     print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "generations=[[Generation(text='\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nRoses are red,\\nViolets are blue,\\nSugar is sweet,\\nAnd so are you.', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 8, 'completion_tokens': 46, 'total_tokens': 54}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('f4ecfa64-014d-4814-9d3a-3b8553d9bab6')), RunInfo(run_id=UUID('11ee152a-daec-49b4-8cfd-8c3e24ff55c9'))]\n",
      "{'token_usage': {'prompt_tokens': 8, 'completion_tokens': 46, 'total_tokens': 54}, 'model_name': 'text-davinci-003'}\n"
     ]
    }
   ],
   "source": [
    "## Simplest way to use LLM is a callable:\n",
    "# pass in a string, get a string completion\n",
    "\n",
    "## __call__\n",
    "llm(\"Tell me joke.\")\n",
    "\n",
    "## generate\n",
    "# response can include things like multiple top responses and other LLM provider-specific information:\n",
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"])\n",
    "print(len(llm_result.generations))\n",
    "# langchain.schema.output.LLMResult\n",
    "print(llm_result)\n",
    "\n",
    "# LLMResult has parameter llm_output - This information is not standardized across providers.\n",
    "print(llm_result.llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What did the fish say when it hit the wall?\n",
      "A: Dam!\n",
      "\n",
      "\n",
      "Q: What did the fish say when he hit the wall?\n",
      "A: Dam!\n",
      "\n",
      "\n",
      "Q: What did the fish say when it hit the wall?\n",
      "A: Dam!\n",
      "\n",
      "\n",
      "Q: Why don't scientists trust atoms? \n",
      "A: Because they make up everything!\n",
      "\n",
      "\n",
      "Q: Why don't scientists trust atoms? \n",
      "A: Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import asyncio\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# async using agenerate method\n",
    "async def async_generate(llm):\n",
    "    res = await llm.agenerate([\"Tell me a joke.\"])\n",
    "    print(res.generations[0][0].text)\n",
    "\n",
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature=.9)\n",
    "    tasks = [async_generate(llm) for _ in range(5)]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "await generate_concurrently()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a \n",
      "\u001b[1mCustomLLM\u001b[0m\n",
      "Params: {'n': 10}\n"
     ]
    }
   ],
   "source": [
    "## Custom LLM\n",
    "# _call method: required\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    n: int\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return prompt[: self.n]\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"n\": self.n}\n",
    "    \n",
    "llm = CustomLLM(n=10)\n",
    "print(llm(\"This is a foobar thing\"))\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
