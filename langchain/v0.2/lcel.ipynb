{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from hydra import compose, initialize\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "with initialize(version_base=None, config_path=\"./config\"):\n",
    "    cfg = compose(config_name=\"properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig\n",
    "from openai import RateLimitError\n",
    "from langchain.chains.base import Chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def build_chat_model(cfg: DictConfig):\n",
    "    model = AzureChatOpenAI(\n",
    "        **cfg.llm.openai\n",
    "    ).configurable_alternatives(\n",
    "        ConfigurableField(id=\"llm_type\"),\n",
    "        default_key=\"openai\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "MAX_ATTEMPT = 3\n",
    "\n",
    "model = build_chat_model(cfg)\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}.\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 1. LLM Simple Usage \n",
    "# # results in underlying API call of:\n",
    "# # openai.AzureOpenAI(..).chat.completions.create(..., logprobs=True)\n",
    "\n",
    "# messages = [\n",
    "#     (\n",
    "#         \"system\",\n",
    "#         \"You are a helpful translator. Translate the user sentence to French.\",\n",
    "#     ),\n",
    "#     (\"human\", \"I love programming.\"),\n",
    "# ]\n",
    "# ai_message = model.invoke(messages, logprobs=True)\n",
    "\n",
    "# 2. LLM Chain Usage with LCEL\n",
    "\n",
    "# To follow the steps along:\n",
    "\n",
    "# 1. We pass in user input on the desired topic as {\"topic\": \"stock\"}\n",
    "# 2. The prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt.\n",
    "# 3. The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object.\n",
    "# 4. Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\n",
    "\n",
    "chain = (\n",
    "    prompt\n",
    "    | model.with_retry(\n",
    "        retry_if_exception_type=(RateLimitError,),\n",
    "        stop_after_attempt=MAX_ATTEMPT,\n",
    "        wait_exponential_jitter=True\n",
    "    ) \n",
    "    | output_parser\n",
    ")\n",
    "response = chain.invoke({\"topic\": \"stock\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[4, 6, 8]\n",
      "{'mul_2': 4, 'mul_5': 10}\n",
      "{'title': 'RunnableLambdaInput'}\n",
      "{'title': 'RunnableParallel<mul_2,mul_5>Output', 'type': 'object', 'properties': {'mul_2': {'title': 'Mul 2'}, 'mul_5': {'title': 'Mul 5'}}}\n",
      "{'mul_2': 4, 'mul_5': 10, 'pass': 2}\n",
      "{'title': 'RunnableLambdaInput'}\n",
      "{'title': 'RunnableParallel<mul_2,mul_5,pass>Output', 'type': 'object', 'properties': {'mul_2': {'title': 'Mul 2'}, 'mul_5': {'title': 'Mul 5'}, 'pass': {'title': 'Pass'}}}\n",
      "This code failed, and will probably be retried!\n",
      "This code failed, and will probably be retried!\n",
      "This code failed, and will probably be retried!\n",
      "This code failed, and will probably be retried!\n",
      "This code failed, and will probably be retried!\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Key Methods - langchain.runnables.base.Runnable\n",
    "\n",
    "#     - **invoke/ainvoke**: Transforms a single input into an output.\n",
    "#     - **batch/abatch**: Efficiently transforms multiple inputs into outputs.\n",
    "#     - **stream/astream**: Streams output from a single input as it's produced.\n",
    "#     - **astream_log**: Streams output and selected intermediate results from an input.\n",
    "\n",
    "#     Built-in optimizations:\n",
    "\n",
    "#     - **Batch**: By default, batch runs invoke() in parallel using a thread pool executor.\n",
    "#       Override to optimize batching.\n",
    "\n",
    "#     - **Async**: Methods with \"a\" suffix are asynchronous. By default, they execute\n",
    "#       the sync counterpart using asyncio's thread pool.\n",
    "#       Override for native async.\n",
    "\n",
    "#     **RunnableSequence** invokes a series of runnables sequentially, with\n",
    "#     one Runnable's output serving as the next's input. Construct using\n",
    "#     the `|` operator or by passing a list of runnables to RunnableSequence.\n",
    "\n",
    "#     **RunnableParallel** invokes runnables concurrently, providing the same input\n",
    "#     to each. Construct it using a dict literal within a sequence or by passing a\n",
    "#     dict to RunnableParallel.\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Runnable Sequence constructed using the | operator\n",
    "sequence = RunnableLambda(lambda x: x+1) | RunnableLambda(lambda x: x*2)\n",
    "print(sequence.invoke(1))\n",
    "print(sequence.batch([1, 2, 3], config={\"max_concurrency\": 5}))\n",
    "\n",
    "# A sequence that contains a RunnableParallel constructed using a dict parallel\n",
    "sequence = RunnableLambda(lambda x: x+1) | {\n",
    "    \"mul_2\": RunnableLambda(lambda x: x*2),\n",
    "    \"mul_5\": RunnableLambda(lambda x: x*5)\n",
    "}\n",
    "print(sequence.invoke(1))\n",
    "print(sequence.input_schema.schema())\n",
    "print(sequence.output_schema.schema()) # RunnableParallel\n",
    "\n",
    "sequence = RunnableLambda(lambda x: x+1) | RunnableParallel(\n",
    "    {\n",
    "        \"mul_2\": lambda x: x*2,\n",
    "        \"mul_5\": RunnableLambda(lambda x: x*5),\n",
    "        \"pass\": RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "print(sequence.invoke(1))\n",
    "print(sequence.input_schema.schema())\n",
    "print(sequence.output_schema.schema()) # RunnableParallel\n",
    "\n",
    "# All Runnables expose additional methods that can be used to modify their behavior\n",
    "# (e.g., add a retry policy, add lifecycle listeners, make them configurable, etc.).\n",
    "\n",
    "import random\n",
    "import random\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def buggy_double(y: int) -> int:\n",
    "    '''Buggy code that will fail 70% of the time'''\n",
    "    if random.random() > 0.3:\n",
    "        print('This code failed, and will probably be retried!')  # noqa: T201\n",
    "        raise ValueError('Triggered buggy code')\n",
    "    return y * 2\n",
    "\n",
    "sequence = (\n",
    "    RunnableLambda(add_one) |\n",
    "    RunnableLambda(buggy_double).with_retry( # Retry on failure\n",
    "        stop_after_attempt=10,\n",
    "        wait_exponential_jitter=False\n",
    "    )\n",
    ")\n",
    "print(sequence.invoke(2))\n",
    "\n",
    "# RunnableParallel with itemgetter when given multiple input keys\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "prompt = \"\"\"original number: {num}\n",
    "Add one nmber: {num_1}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "sequence = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"num_1\": itemgetter(\"num\") | RunnableLambda(lambda x: x+1),\n",
    "            \"num\": itemgetter(\"num\")\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    ")\n",
    "\n",
    "prompt_val = sequence.invoke({\"num\": 1, \"UNUSED_PARAM\": 999})\n",
    "print(prompt_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
