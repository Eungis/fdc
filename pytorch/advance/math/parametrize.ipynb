{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2041, 0.5863, 0.9884],\n",
      "        [0.5863, 0.8330, 0.8776],\n",
      "        [0.9884, 0.8776, 0.3282]])\n",
      "torch.Size([8, 3])\n",
      "Type of x: <class 'torch.nn.parameter.Parameter'> = nn.Parameter\n",
      "x equals as `source for making symmtric matrix` below\n",
      "ParametrizedLinear(\n",
      "  in_features=3, out_features=3, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): Symmetric()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Type of x: <class 'torch.nn.parameter.Parameter'> = nn.Parameter\n",
      "x equals as `source for making symmtric matrix` below\n",
      "tensor([[-0.0467,  0.2155, -0.4387],\n",
      "        [ 0.2155,  0.1236, -0.3720],\n",
      "        [-0.4387, -0.3720, -0.5221]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.0000,  0.1193, -0.0805],\n",
      "        [-0.1193,  0.0000, -0.1297],\n",
      "        [ 0.0805,  0.1297,  0.0000]], grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.0000, -0.0061, -0.0668],\n",
      "        [ 0.0061,  0.0000,  0.0471],\n",
      "        [ 0.0668, -0.0471,  0.0000]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "# Assume that we want to have a square linear layer with symmetric weights, that is, with weights X such that X = Xáµ€.\n",
    "# One way to do so is to copy the upper-triangular part of the matrix into its lower-triangular part.\n",
    "\n",
    "\n",
    "# Implementing parametrizations by hand\n",
    "def symmetric(X):\n",
    "    return X.triu() + X.triu(1).transpose(-1, -2)\n",
    "\n",
    "\n",
    "X = torch.rand(3, 3)\n",
    "A = symmetric(X)\n",
    "assert torch.allclose(A, A.T)  # A is symmetric\n",
    "print(A)\n",
    "\n",
    "\n",
    "# Implement \"a linear layer with symmetric weights\"\n",
    "class LinearSymmetric(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(n_features, n_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        A = symmetric(self.weight)\n",
    "        return x @ A\n",
    "\n",
    "\n",
    "layer = LinearSymmetric(n_features=3)\n",
    "out = layer(torch.rand(8, 3))\n",
    "print(out.shape)\n",
    "\n",
    "# Promblem comes in.\n",
    "\n",
    "# 1. It does not separate the layer and the parametrization. If the parametrization were more difficult, we would have to rewrite its code for each layer that we want to use it in.\n",
    "# 2. It recomputes the parametrization every time we use the layer.\n",
    "# If we use the layer several times during the forward pass, (imagine the recurrent kernel of an RNN), it would compute the same A every time that the layer is called.\n",
    "\n",
    "# Parametrization can solve all these problems as well as others.\n",
    "\n",
    "\n",
    "class Symmetric(nn.Module):\n",
    "    def forward(self, x):\n",
    "        print(f\"Type of x: {type(x)} = nn.Parameter\")\n",
    "        print(\"x equals as `source for making symmtric matrix` below\")\n",
    "        return x.triu() + x.triu(1).transpose(-1, -2)\n",
    "\n",
    "\n",
    "layer = nn.Linear(3, 3)\n",
    "# replace `layer.weight` attribute with Symmetric()\n",
    "# it inputs |3, 3| weights of original Linear layer, put into Symmetric.forward(),\n",
    "# and do things as below:\n",
    "# self.weight = Symmetric()(self.weight)\n",
    "parametrize.register_parametrization(layer, \"weight\", Symmetric())\n",
    "print(layer)\n",
    "\n",
    "A = layer.weight\n",
    "assert torch.allclose(A, A.T)\n",
    "print(A)\n",
    "\n",
    "\n",
    "# Similar case example (Skew)\n",
    "# We can do the same thing with any other layer.\n",
    "# For example, we can create a CNN with skew-symmetric kernels.\n",
    "# We use a similar parametrization, copying the upper-triangular part with signs reversed into the lower-triangular part.\n",
    "class Skew(nn.Module):\n",
    "    def forward(self, X):\n",
    "        A = X.triu(1)\n",
    "        return A - A.transpose(-1, -2)\n",
    "\n",
    "\n",
    "cnn = nn.Conv2d(in_channels=5, out_channels=8, kernel_size=3)\n",
    "parametrize.register_parametrization(cnn, \"weight\", Skew())\n",
    "\n",
    "# Print a few kernels\n",
    "print(cnn.weight[0, 1])\n",
    "print(cnn.weight[2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
