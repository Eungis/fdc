{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# loading script\n",
    "# csv, text, json, pandas\n",
    "\n",
    "# question-answering datasets: all the text stored in a data field\n",
    "# load the dataset by specifying the field argument as follows:\n",
    "\n",
    "# By default, loading local files creates a DatasetDict object with a train split.\n",
    "# squad_ko_dataset = load_dataset(\"json\", data_files=\"./data/KorQuAD_v1.0_train.json\", field=\"data\")\n",
    "# squad_ko_dataset[\"train\"][0]\n",
    "\n",
    "# include both the train and test splits in a single DatasetDict\n",
    "data_files = {\n",
    "    \"train\": \"./data/KorQuAD_v1.0_train.json\",\n",
    "    \"test\": \"./data/KorQuAD_v1.0_dev.json\",\n",
    "}\n",
    "# data_files argument is quite flexible: ex. \"*.json\" - Unix shell pattern is available\n",
    "# for more detailed information, refer to the documentation:\n",
    "# https://huggingface.co/docs/datasets/loading#local-and-remote-files\n",
    "squad_ko_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['paragraphs', 'title'],\n",
      "        num_rows: 1420\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['paragraphs', 'title'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(squad_ko_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'formal': ['네, 기말 시험 두개가 오늘 있어서 공부하느라 잠을 못 잤어요.', None, '세계여행을 하는데 가장 기대되는 나라가 있나요?'], 'informal': ['응, 기말 시험 두개가 오늘 있어서 공부하느라 잠을 못 잤어.', None, '세계여행을 하는데 가장 기대되는 나라가 있어?'], 'android': [None, None, '세계여행. 하기. 기대됨. 국가. 있는가.'], 'azae': [None, None, None], 'chat': ['ㅇㅇ 기말 시험 두개가 오늘 있어서 공부하느라 잠을 못 잠', None, '세계여행 하는데 가장 기대되는 나라 있음?'], 'choding': ['ㅇㅇ 기말 셤 두개가 오늘이라 공부하다가 잠을 못 잠', None, '세계여행 하는데 젤 기대되는 나라 잇음?'], 'emoticon': [None, None, '세계여행 하는데 가장 기대되는 나라 있어? (⊙_⊙)？'], 'enfp': [None, None, '세계여행하는데, 가장 기대되는 나라 있어??'], 'gentle': [None, None, '세계여행을 하는데 가장 기대되는 나라가 있으십니까?'], 'halbae': [None, None, '세계여행 하는디 가장 기대가 되는 나라가 있으신가?...'], 'halmae': [None, None, None], 'joongding': ['ㅇ 기말시험 두개 오늘 있어서 공부하느라 잠 못잤음', None, '세계여행 하는데 어디가 제일 기대되냐 ㅋ'], 'king': [None, None, '세계여행을 하는 데 가장 기대되는 나라가 있는가?'], 'naruto': [None, None, '세계여행 하는데 가장 기대가 되는 나라 있냐니깐!'], 'seonbi': [None, None, '세계여행 하는데 가장 기대가 되는 나라가 있소? '], 'sosim': [None, None, '세계여행 하는데 가장 기대되는 나라 있어 혹시..?'], 'translator': [None, None, None]}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"csv\", data_files=\"./data/smilestyle_dataset.tsv\", delimiter=\"\\t\"\n",
    ")\n",
    "dataset_sample = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "print(dataset_sample[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요. 저는 고양이 6마리 키워요.', '고양이를 6마리나요? 키우는거 안 힘드세요?']\n"
     ]
    }
   ],
   "source": [
    "for split in dataset.keys():\n",
    "    # train, test, etc.\n",
    "    print(dataset[split].unique(\"formal\")[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(original_column_name=\"formal\", new_column_name=\"Formal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter nones\n",
    "dataset = dataset.filter(lambda x: x[\"android\"] is not None)\n",
    "\n",
    "\n",
    "# normalize teh field\n",
    "def lowercase_android(example):\n",
    "    return {\"android\": example[\"android\"].lower()}\n",
    "\n",
    "\n",
    "dataset = dataset.map(lowercase_android)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column\n",
    "def compute_android_length(example):\n",
    "    return {\"android_length\": len(example[\"android\"].split())}\n",
    "\n",
    "\n",
    "dataset = dataset.map(compute_android_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"train\"].sort(\"android_length\", reverse=True)[:2]\n",
    "dataset = dataset.filter(lambda x: x[\"android_length\"] > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 98}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "field = \"android\"\n",
    "dataset = dataset.map(lambda x: {field: html.unescape(x[field])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch execution - faster than the batched=False\n",
    "# batch size is configurable but defaults to 1,000.\n",
    "\n",
    "# It's especially useful when you use FastTokenizer (AutoTokenizer will use fast tokenizers as default)\n",
    "# FastTokenizer achieves such a speedup because behind the scenes the tokenization code is executed in Rust,\n",
    "# which is language that makes it easy parallelize code execution.\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\n",
    "        field: [html.unescape(o) for o in x[field]]\n",
    "    },  # list comprehension due to batched\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-processing is available (not backed by Rust)\n",
    "# In general, we don’t recommend using Python multiprocessing for fast tokenizers with batched=True.\n",
    "dataset = dataset.map(\n",
    "    lambda x: {field: [html.unescape(o) for o in x[field]]},\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 6231, 4176, 18, 3], [2, 2296, 4034, 2048, 3], [2, 18, 3311, 4176, 3], [2, 18, 6288, 18, 3], [2, 3093, 4006, 18, 3], [2, 14793, 18, 9193, 3], [2, 4325, 18, 7926, 3], [2, 18, 11122, 18, 3], [2, 7265, 7796, 18, 3], [2, 7899, 18, 6333, 3], [2, 18, 6820, 18, 3], [2, 7317, 4031, 18, 3], [2, 6288, 18, 27305, 3], [2, 4297, 18, 10652, 3], [2, 4176, 18, 2897, 3], [2, 16465, 18, 7202, 3], [2, 18, 3019, 4176, 3], [2, 18, 6841, 18, 3], [2, 7856, 18, 2884, 3], [2, 4498, 8094, 18, 3], [2, 2897, 16465, 18, 3]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def tokenize_and_split(examples: dict):\n",
    "    return tokenizer(\n",
    "        examples[\"android\"],\n",
    "        truncation=True,\n",
    "        max_length=5,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "print(tokenize_and_split(dataset[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_split,\n",
    "    batched=True,\n",
    "    # That doesn’t work for a Dataset,\n",
    "    # so we need to either remove the columns from the old dataset or make them the same size as they are in the new dataset.\n",
    "    # (due to return_overflowing_tokens=True, the original length is changed.)\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def tokenize_and_split(examples: dict):\n",
    "    assert len(examples) == len(dataset.column_names[\"train\"])\n",
    "    assert len(examples[\"android\"]) == dataset.num_rows[\"train\"]\n",
    "    result = tokenizer(\n",
    "        examples[\"android\"],\n",
    "        truncation=True,\n",
    "        max_length=5,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for column, sentences in examples.items():\n",
    "        result[column] = [sentences[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenized_dataset = dataset.map(tokenize_and_split, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset.num_rows[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "# it switches to another format without affecting the underlying data format, which is Apache Arrow.\n",
    "tokenized_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tokenized_dataset[\"train\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Dataset Object by using the Dataset.from_pandas() function as follows:\n",
    "from datasets import Dataset\n",
    "\n",
    "# Do something we want with pandas\n",
    "train_df = train_df.rename(columns={\"Formal\": \"formal\"}).reset_index(drop=True)\n",
    "train_dataset = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets provides a Dataset.train_test_split() function that is based on the famous functionality from scikit-learn.\n",
    "dataset_clean = tokenized_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Formal', 'informal', 'android', 'azae', 'chat', 'choding', 'emoticon', 'enfp', 'gentle', 'halbae', 'halmae', 'joongding', 'king', 'naruto', 'seonbi', 'sosim', 'translator', 'android_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 796\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Formal', 'informal', 'android', 'azae', 'chat', 'choding', 'emoticon', 'enfp', 'gentle', 'halbae', 'halmae', 'joongding', 'king', 'naruto', 'seonbi', 'sosim', 'translator', 'android_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 199\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the default `test` split to `validation`\n",
    "dataset_clean[\"validation\"] = dataset_clean.pop(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "\n",
    "# Arrow: Dataset.save_to_disk()\n",
    "# CSV: Dataset.to_csv()\n",
    "# JSON: Dataset.to_json()\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_clean.save_to_disk(\"./data/smilestyle_dataset\")\n",
    "dataset_clean = load_from_disk(\"./data/smilestyle_dataset\")\n",
    "\n",
    "# # save as json\n",
    "# for split, dataset in dataset.items():\n",
    "#     dataset.to_json(f\"dataset-{split}.jsonl\")\n",
    "\n",
    "# # load from json\n",
    "# data_files = {\n",
    "#     \"train\": \"./data/dataset-train.jsonl\",\n",
    "#     \"validation\": \"./data/dataset-validation.jsonl\"\n",
    "# }\n",
    "# dataset_clean = load_dataset(\"json\", data_file=data_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
