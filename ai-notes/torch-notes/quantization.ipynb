{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.utils.parametrize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5177, 0.4886, 0.2425],\n",
      "        [0.4886, 0.3903, 0.7523],\n",
      "        [0.2425, 0.7523, 0.3229]])\n",
      "torch.Size([8, 3])\n",
      "Type of x: <class 'torch.nn.parameter.Parameter'> = nn.Parameter\n",
      "x equals as `source for making symmtric matrix` below\n",
      "ParametrizedLinear(\n",
      "  in_features=3, out_features=3, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): Symmetric()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Type of x: <class 'torch.nn.parameter.Parameter'> = nn.Parameter\n",
      "x equals as `source for making symmtric matrix` below\n",
      "tensor([[-0.2961,  0.5623,  0.2940],\n",
      "        [ 0.5623, -0.3487,  0.3402],\n",
      "        [ 0.2940,  0.3402,  0.1318]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.0000,  0.0506,  0.0236],\n",
      "        [-0.0506,  0.0000,  0.1251],\n",
      "        [-0.0236, -0.1251,  0.0000]], grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.0000,  0.1292,  0.0750],\n",
      "        [-0.1292,  0.0000, -0.1343],\n",
      "        [-0.0750,  0.1343,  0.0000]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "# Assume that we want to have a square linear layer with symmetric weights, that is, with weights X such that X = Xáµ€.\n",
    "# One way to do so is to copy the upper-triangular part of the matrix into its lower-triangular part.\n",
    "\n",
    "\n",
    "# Implementing parametrizations by hand\n",
    "def symmetric(X):\n",
    "    return X.triu() + X.triu(1).transpose(-1, -2)\n",
    "\n",
    "\n",
    "X = torch.rand(3, 3)\n",
    "A = symmetric(X)\n",
    "assert torch.allclose(A, A.T)  # A is symmetric\n",
    "print(A)\n",
    "\n",
    "\n",
    "# Implement \"a linear layer with symmetric weights\"\n",
    "class LinearSymmetric(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(n_features, n_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        A = symmetric(self.weight)\n",
    "        return x @ A\n",
    "\n",
    "\n",
    "layer = LinearSymmetric(n_features=3)\n",
    "out = layer(torch.rand(8, 3))\n",
    "print(out.shape)\n",
    "\n",
    "# Promblem comes in.\n",
    "\n",
    "# 1. It does not separate the layer and the parametrization. If the parametrization were more difficult, we would have to rewrite its code for each layer that we want to use it in.\n",
    "# 2. It recomputes the parametrization every time we use the layer.\n",
    "# If we use the layer several times during the forward pass, (imagine the recurrent kernel of an RNN), it would compute the same A every time that the layer is called.\n",
    "\n",
    "# Parametrization can solve all these problems as well as others.\n",
    "\n",
    "\n",
    "class Symmetric(nn.Module):\n",
    "    def forward(self, x):\n",
    "        print(f\"Type of x: {type(x)} = nn.Parameter\")\n",
    "        print(\"x equals as `source for making symmtric matrix` below\")\n",
    "        return x.triu() + x.triu(1).transpose(-1, -2)\n",
    "\n",
    "\n",
    "layer = nn.Linear(3, 3)\n",
    "# replace `layer.weight` attribute with Symmetric()\n",
    "# it inputs |3, 3| weights of original Linear layer, put into Symmetric.forward(),\n",
    "# and do things as below:\n",
    "# self.weight = Symmetric()(self.weight)\n",
    "parametrize.register_parametrization(layer, \"weight\", Symmetric())\n",
    "print(layer)\n",
    "\n",
    "A = layer.weight\n",
    "assert torch.allclose(A, A.T)\n",
    "print(A)\n",
    "\n",
    "\n",
    "# Similar case example (Skew)\n",
    "# We can do the same thing with any other layer.\n",
    "# For example, we can create a CNN with skew-symmetric kernels.\n",
    "# We use a similar parametrization, copying the upper-triangular part with signs reversed into the lower-triangular part.\n",
    "class Skew(nn.Module):\n",
    "    def forward(self, X):\n",
    "        A = X.triu(1)\n",
    "        return A - A.transpose(-1, -2)\n",
    "\n",
    "\n",
    "cnn = nn.Conv2d(in_channels=5, out_channels=8, kernel_size=3)\n",
    "parametrize.register_parametrization(cnn, \"weight\", Skew())\n",
    "\n",
    "# Print a few kernels\n",
    "print(cnn.weight[0, 1])\n",
    "print(cnn.weight[2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.ModuleDict'> 1\n",
      "ModuleDict(\n",
      "  (weight): ParametrizationList(\n",
      "    (0): Symmetric()\n",
      "  )\n",
      ")\n",
      "True\n",
      "<class 'torch.nn.utils.parametrize.ParametrizedLinear'> <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "# Check if the layer is parametrize\n",
    "parametrizations = getattr(layer, \"parametrizations\", None)\n",
    "print(type(parametrizations), len(parametrizations))\n",
    "print(parametrizations)\n",
    "\n",
    "# use function below\n",
    "from torch.nn.utils.parametrize import is_parametrized, type_before_parametrizations\n",
    "\n",
    "print(is_parametrized(layer))\n",
    "print(type(layer), type_before_parametrizations(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.quantization.fuse_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18(num_classes=10, weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- layer 1 ---------\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "--------- layer 1 > first module > conv1 ---------\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "--------- named_modules ---------\n",
      "conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "--------- named_modules vs. __getattr__---------\n",
      "True\n",
      "--------- Fuse modules ---------\n",
      "(<class 'torch.nn.modules.conv.Conv2d'>, <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, <class 'torch.nn.modules.activation.ReLU'>)\n",
      "ConvReLU2d(\n",
      "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n",
      "[ConvReLU2d(\n",
      "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (1): ReLU(inplace=True)\n",
      "), Identity(), Identity()]\n"
     ]
    }
   ],
   "source": [
    "# Models to fuse: conv1, bn1, relu\n",
    "\n",
    "# Function: get_submodule\n",
    "print(\"--------- layer 1 ---------\")\n",
    "print(model.get_submodule(\"layer1\"))\n",
    "\n",
    "print(\"--------- layer 1 > first module > conv1 ---------\")\n",
    "print(model.get_submodule(\"layer1\")[0].get_submodule(\"conv1\"))\n",
    "\n",
    "# Function: named_submodules\n",
    "named_modules = list(model.named_modules())\n",
    "# First element would be: \"\", entire model\n",
    "# next element would be continued from 1\n",
    "print(\"--------- named_modules ---------\")\n",
    "print(named_modules[1][0], named_modules[1][1])\n",
    "\n",
    "# Function: get directly\n",
    "print(\"--------- named_modules vs. __getattr__---------\")\n",
    "print(getattr(model, \"conv1\") == named_modules[1][1])\n",
    "\n",
    "\n",
    "# Function: fuse modules\n",
    "print(\"--------- Fuse modules ---------\")\n",
    "from typing import List, Optional\n",
    "from torch.ao.quantization.fuse_modules import fuse_known_modules\n",
    "from torch.ao.quantization.fuser_method_mappings import get_fuser_method\n",
    "\n",
    "model.eval()\n",
    "\n",
    "modules_to_fuse = [\"conv1\", \"bn1\", \"relu\"]\n",
    "mod_list = [model.get_submodule(mod) for mod in modules_to_fuse]\n",
    "types = tuple(type_before_parametrizations(m) for m in mod_list)\n",
    "print(types)\n",
    "fuser_method = get_fuser_method(types)\n",
    "\n",
    "is_qat = False\n",
    "new_mod: List[Optional[nn.Module]] = [None] * len(mod_list)\n",
    "fused = fuser_method(is_qat, *mod_list)\n",
    "print(fused)\n",
    "\n",
    "# NOTE: forward hooks not processed in the two following for loops will be lost after the fusion\n",
    "# Move pre forward hooks of the base module to resulting fused module\n",
    "for handle_id, pre_hook_fn in mod_list[0]._forward_pre_hooks.items():\n",
    "    fused.register_forward_pre_hook(pre_hook_fn)\n",
    "    del mod_list[0]._forward_pre_hooks[handle_id]\n",
    "\n",
    "# Move post forward hooks of the last module to resulting fused module\n",
    "for handle_id, hook_fn in mod_list[-1]._forward_hooks.items():\n",
    "    fused.register_forward_hook(hook_fn)\n",
    "    del mod_list[-1]._forward_hooks[handle_id]\n",
    "\n",
    "# The first element in the output module list performs the fused operation.\n",
    "# The rest of the elements are set to nn.Identity()\n",
    "new_mod[0] = fused\n",
    "for i in range(1, len(mod_list)):\n",
    "    identity = nn.Identity()\n",
    "    identity.training = mod_list[0].training\n",
    "    new_mod[i] = identity\n",
    "print(new_mod)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
