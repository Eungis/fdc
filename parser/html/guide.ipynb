{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Guide on HTMLCleanser and HTMLSplitter\n",
    ":: This guide introduces way to use the HTMLCleanser and HTMLSplitter.\n",
    "\n",
    "- Its purpose is on introducing the HTMLCleanser to preprocess the html (e.g. cleanse out invalid html tags and attributes).\n",
    "- Its purpose is on introducing the HTMLSplitter to split the documents before indexing the documents using IR model to perform RAG.\n",
    "- It supposes that your document is of the format `html`.\n",
    "\n",
    "\n",
    "\n",
    ":: Main modules: [cleanser.py, splitter.py]\n",
    "\n",
    "- The system pre-processes files in the html format. If your document is in another format, please convert it to an HTML file beforehand.\n",
    "\n",
    "- The `HTMLCleanser` in cleanser.py removes invalid tags and attributes within the BeautifulSoup object.\n",
    "- The `HTMLSplitter` in splitter.py divides the file into several documents without altering the HTML contents.\n",
    "    - When developing an AI model or utilizing the LLM, the existence of token_max can make it challenging to input the entire document into the model.\n",
    "    - Consequently, you may have been splitting the documents to avoid exceeding the token_max, using strategies such as doc_stride, etc.\n",
    "    - If you simply truncate the document based on the token length, the content may be chopped off, resulting in a loss of context.\n",
    "    - The `HTMLSplitter` takes into account the token_max when splitting the file, ensuring that the HTML file is divided without compromising context.\n",
    "\n",
    "- [as-is] The contents (e.g., tables) inside the document were truncated without considering the context due to the maximum token limit imposed by the AI model.\n",
    "\n",
    "- [to-be] The contents (e.g., tables) inside the document will be split without losing context and structure, with a guarantee that the splitted chunks do not exceed the maximum tokens.\n",
    "\n",
    "    Firstly, split the target file into a set of documents before indexing, while considering the maximum token limit accepted by the model.\n",
    "\n",
    "    Secondly, insert the splitted documents into the model as usual.\n",
    "\n",
    "- For detailed usage instructions of the modules, kindly consult the `guide.ipynb` file located within the html directory.\n",
    "\n",
    "[TODO]\n",
    "- Add more funtionality in HTMLSplitter._split_chunk \n",
    "- Think of how to handle the context window \n",
    "\n",
    "```\n",
    "- Writer: Eungi Cho\n",
    "- Last update: 23.11.21\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-21 17:12:25.196\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36m_html.cleanser\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mNo valid_tags or valid_attrs provided.\n",
      "                    Initialize the HTMLCleanser with default ones:\n",
      "                    - valid_tags: ['table', 'tr', 'td', 'th']\n",
      "                    - valid_attrs: ['rowspan', 'colspan']\u001b[0m\n",
      "\u001b[32m2023-11-21 17:12:25.197\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36m_html.cleanser\u001b[0m:\u001b[36madd_valid_tags\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mvalid_tags: ['a', 'p', 'title', 'img', 'tr', 'td', 'table', 'span', 'th']\u001b[0m\n",
      "\u001b[32m2023-11-21 17:12:25.198\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36m_html.cleanser\u001b[0m:\u001b[36mremove_valid_tags\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mvalid_tags: ['a', 'p', 'title', 'img', 'tr', 'td', 'table', 'th']\u001b[0m\n",
      "\u001b[32m2023-11-21 17:12:25.199\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36m_html.cleanser\u001b[0m:\u001b[36madd_valid_attrs\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mvalid_attrs: ['src', 'alt', 'colspan', 'href', 'font', 'rowspan']\u001b[0m\n",
      "\u001b[32m2023-11-21 17:12:25.200\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36m_html.cleanser\u001b[0m:\u001b[36mremove_valid_attrs\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mvalid_attrs: ['src', 'alt', 'colspan', 'href', 'rowspan']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from _html.cleanser import HTMLCleanser\n",
    "\n",
    "# suppose you have the html file as below:\n",
    "txt = \"\"\"\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Header 1</th>\n",
    "            <th>Header 2</th>\n",
    "            <th>Header 3</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Data 1</td>\n",
    "            <td>Data 2</td>\n",
    "            <td>Data 3</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Data 4</td>\n",
    "            <td>Data 5</td>\n",
    "            <td>Data 6</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\"\"\"\n",
    "# get soup object by specifying the parser\n",
    "soup = BeautifulSoup(txt, \"lxml\")\n",
    "\n",
    "# usage of cleanser: it will print out default valid_tags and valid_attributes\n",
    "# to leave in the text when it is initialized.\n",
    "cleanser = HTMLCleanser()\n",
    "# add or remove any tag as your pleases.\n",
    "cleanser.add_valid_tags([\"p\", \"img\", \"a\", \"span\", \"title\"])\n",
    "cleanser.remove_valid_tags([\"span\"])\n",
    "# add or remove any attributes as your pleases.\n",
    "cleanser.add_valid_attrs([\"href\", \"src\", \"alt\", \"font\"])\n",
    "cleanser.remove_valid_attrs([\"font\"])\n",
    "\n",
    "# cleanse your html first.\n",
    "soup = cleanser.cleanse_html(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _html.splitter import HTMLSplitter\n",
    "\n",
    "splitter = HTMLSplitter(soup=soup, length_func=len, token_max=200)\n",
    "chunks = splitter.get_chunks()\n",
    "chunks = splitter.split_chunks(chunks)\n",
    "documents = splitter.make_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2\n",
      "----------- Page 0 | Tokens 199 -----------\n",
      "<table>\n",
      " <tr>\n",
      "  <th>\n",
      "   Header 1\n",
      "  </th>\n",
      "  <th>\n",
      "   Header 2\n",
      "  </th>\n",
      "  <th>\n",
      "   Header 3\n",
      "  </th>\n",
      " </tr>\n",
      " <tr>\n",
      "  <td>\n",
      "   Data 1\n",
      "  </td>\n",
      "  <td>\n",
      "   Data 2\n",
      "  </td>\n",
      "  <td>\n",
      "   Data 3\n",
      "  </td>\n",
      " </tr>\n",
      "</table>\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "----------- Page 1 | Tokens 199 -----------\n",
      "<table>\n",
      " <tr>\n",
      "  <th>\n",
      "   Header 1\n",
      "  </th>\n",
      "  <th>\n",
      "   Header 2\n",
      "  </th>\n",
      "  <th>\n",
      "   Header 3\n",
      "  </th>\n",
      " </tr>\n",
      " <tr>\n",
      "  <td>\n",
      "   Data 4\n",
      "  </td>\n",
      "  <td>\n",
      "   Data 5\n",
      "  </td>\n",
      "  <td>\n",
      "   Data 6\n",
      "  </td>\n",
      " </tr>\n",
      "</table>\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents: {len(documents)}\")\n",
    "\n",
    "for doc in documents:\n",
    "    print(f\"----------- Page {doc.page} | Tokens {len(doc.page_content)} -----------\")\n",
    "    print(doc.page_content)\n",
    "    print(f\"----------------------------------------\")\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
