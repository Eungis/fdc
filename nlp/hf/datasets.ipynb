{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# loading script\n",
    "# csv, text, json, pandas\n",
    "\n",
    "# question-answering datasets: all the text stored in a data field\n",
    "# load the dataset by specifying the field argument as follows:\n",
    "\n",
    "# By default, loading local files creates a DatasetDict object with a train split.\n",
    "# squad_ko_dataset = load_dataset(\"json\", data_files=\"./data/KorQuAD_v1.0_train.json\", field=\"data\")\n",
    "# squad_ko_dataset[\"train\"][0]\n",
    "\n",
    "# include both the train and test splits in a single DatasetDict\n",
    "data_files = {\n",
    "    \"train\": \"./data/KorQuAD_v1.0_train.json\",\n",
    "    \"test\": \"./data/KorQuAD_v1.0_dev.json\",\n",
    "}\n",
    "# data_files argument is quite flexible: ex. \"*.json\" - Unix shell pattern is available\n",
    "# for more detailed information, refer to the documentation:\n",
    "# https://huggingface.co/docs/datasets/loading#local-and-remote-files\n",
    "squad_ko_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_ko_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"csv\", data_files=\"./data/smilestyle_dataset.tsv\", delimiter=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "print(dataset_sample[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset.keys():\n",
    "    # train, test, etc.\n",
    "    print(dataset[split].unique(\"formal\")[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(original_column_name=\"formal\", new_column_name=\"Formal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter nones\n",
    "dataset = dataset.filter(lambda x: x[\"android\"] is not None)\n",
    "\n",
    "\n",
    "# normalize teh field\n",
    "def lowercase_android(example):\n",
    "    return {\"android\": example[\"android\"].lower()}\n",
    "\n",
    "\n",
    "dataset = dataset.map(lowercase_android)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column\n",
    "def compute_android_length(example):\n",
    "    return {\"android_length\": len(example[\"android\"].split())}\n",
    "\n",
    "\n",
    "dataset = dataset.map(compute_android_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"train\"].sort(\"android_length\", reverse=True)[:2]\n",
    "dataset = dataset.filter(lambda x: x[\"android_length\"] > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "field = \"android\"\n",
    "dataset = dataset.map(lambda x: {field: html.unescape(x[field])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch execution - faster than the batched=False\n",
    "# batch size is configurable but defaults to 1,000.\n",
    "\n",
    "# It's expecially useful when you use FastTokenizer (AutoTokenizer will use fast tokenizers as default)\n",
    "# FastTokenizer achieves such a speedup because behind the scenes the tokenization code is executed in Rust,\n",
    "# which is language that makes it easy parallelize code execution.\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\n",
    "        field: [html.unescape(o) for o in x[field]]\n",
    "    },  # list comprehension due to batched\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-processing is available (not backed by Rust)\n",
    "# In general, we don’t recommend using Python multiprocessing for fast tokenizers with batched=True.\n",
    "dataset = dataset.map(\n",
    "    lambda x: {field: [html.unescape(o) for o in x[field]]},\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def tokenize_and_split(examples: dict):\n",
    "    return tokenizer(\n",
    "        examples[\"android\"],\n",
    "        truncation=True,\n",
    "        max_length=5,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "print(tokenize_and_split(dataset[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_split,\n",
    "    batched=True,\n",
    "    # That doesn’t work for a Dataset,\n",
    "    # so we need to either remove the columns from the old dataset or make them the same size as they are in the new dataset.\n",
    "    # (due to return_overflowing_tokens=True, the original length is changed.)\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def tokenize_and_split(examples: dict):\n",
    "    result = tokenizer(\n",
    "        examples[\"android\"],\n",
    "        truncation=True,\n",
    "        max_length=5,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenized_dataset = dataset.map(tokenize_and_split, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "# it switches to another format without affecting the underlying data format, which is Apache Arrow.\n",
    "tokenized_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tokenized_dataset[\"train\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do something we want with pandas\n",
    "train_df = train_df.rename(columns={\"Formal\": \"formal\"}).reset_index(drop=True)\n",
    "\n",
    "# create a new Dataset Object by using the Dataset.from_pandas() function as follows:\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets provides a Dataset.train_test_split() function that is based on the famous functionality from scikit-learn.\n",
    "dataset_clean = tokenized_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the default `test` split to `validation`\n",
    "dataset_clean[\"validation\"] = dataset_clean.pop(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "\n",
    "# Arrow: Dataset.save_to_disk()\n",
    "# CSV: Dataset.to_csv()\n",
    "# JSON: Dataset.to_json()\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_clean.save_to_disk(\"./data/smilestyle_dataset\")\n",
    "dataset_clean = load_from_disk(\"./data/smilestyle_dataset\")\n",
    "\n",
    "# # save as json\n",
    "# for split, dataset in dataset.items():\n",
    "#     dataset.to_json(f\"dataset-{split}.jsonl\")\n",
    "\n",
    "# # load from json\n",
    "# data_files = {\n",
    "#     \"train\": \"./data/dataset-train.jsonl\",\n",
    "#     \"validation\": \"./data/dataset-validation.jsonl\"\n",
    "# }\n",
    "# dataset_clean = load_dataset(\"json\", data_file=data_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
