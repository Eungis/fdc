{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# in order to import the modules located at the root directory\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from importlib import reload  # Not needed in Python 2\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(\n",
    "    format=\"%(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "NumExpr defaulting to 8 threads.\n",
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "Number of sessions: 236\n"
     ]
    }
   ],
   "source": [
    "from mrs.dataset import SessionBuilder, PostDataset, PostDatasetCollator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "builder = SessionBuilder(style=\"formal\")\n",
    "post_dataset = PostDataset(builder)\n",
    "\n",
    "post_dataloader = DataLoader(\n",
    "    post_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=PostDatasetCollator(\n",
    "        pad_idx=post_dataset.tokenizer.pad_token_id, max_length=99999\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(post_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keys in the batch dictionary: ['mlm_inputs', 'urc_inputs']\n"
     ]
    }
   ],
   "source": [
    "LOGGER.info(f\"Keys in the batch dictionary: {list(batch.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta Model for MLM training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, RobertaForMaskedLM, RobertaModel\n",
    "\n",
    "# same as tokenizer in PostDataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "special_tokens = {\"sep_token\": \"<SEP>\"}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# ways to load roberta model\n",
    "model_1 = RobertaForMaskedLM.from_pretrained(\"klue/roberta-base\")\n",
    "model_2 = RobertaModel.from_pretrained(\"klue/roberta-base\")\n",
    "model_3 = AutoModel.from_pretrained(\"klue/roberta-base\")\n",
    "\n",
    "# resize token embedding\n",
    "model_1.resize_token_embeddings(len(post_dataset.tokenizer))\n",
    "model_2.resize_token_embeddings(len(post_dataset.tokenizer))\n",
    "model_3.resize_token_embeddings(len(post_dataset.tokenizer))\n",
    "\n",
    "model_1.eval()\n",
    "model_2.eval()\n",
    "model_3.eval()\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'hidden_states'])\n"
     ]
    }
   ],
   "source": [
    "# in order to reconstruct the mask tokens, the dimentsion should be same as vocabulary size.\n",
    "result_1 = model_1(batch[\"mlm_inputs\"][\"corrupt_tokens\"], output_hidden_states=True)\n",
    "print(result_1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 101, 32001]) torch.Size([2, 101, 768])\n"
     ]
    }
   ],
   "source": [
    "print(result_1.logits.shape, result_1.hidden_states[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])\n"
     ]
    }
   ],
   "source": [
    "result_2 = model_2(batch[\"mlm_inputs\"][\"corrupt_tokens\"], output_hidden_states=True)\n",
    "print(result_2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 101, 768])\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "print(result_2.last_hidden_state.shape)\n",
    "print(result_2.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "\n",
    "class PostTrainModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        # defined in huggingface PreTrainedTokenizer\n",
    "        # input_size = len(tokenizer) = tokenizer.vocab_size + len(tokenizer.added_tokens_encoder)\n",
    "\n",
    "        super(PostTrainModel, self).__init__()\n",
    "        self.model = RobertaForMaskedLM.from_pretrained(\"klue/roberta-base\")\n",
    "        self.model.resize_token_embeddings(input_size)\n",
    "\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "        self.generator = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        corrupt_tokens,\n",
    "        corrupt_mask_positions,\n",
    "        mlm_attentions,\n",
    "        urc_inputs,\n",
    "        urc_attentions,\n",
    "    ):\n",
    "        # |corrupt_tokens| = (bs, length)\n",
    "        # |corrupt_mask_positions| = List[Tensor]; Length(List)=bs\n",
    "        # |mlm_attentions| = (bs, length)\n",
    "        # |urc_inputs| = (bs*3, length)\n",
    "        # |urc_attentions| = (bs*3, length)\n",
    "\n",
    "        # forward for MLM\n",
    "        corrupt_outputs = self.model(corrupt_tokens, attention_mask=mlm_attentions)[\n",
    "            \"logits\"\n",
    "        ]\n",
    "        # |corrupt_outputs| = (bs, length, vocab_size)\n",
    "\n",
    "        corrupt_mask_outputs = []\n",
    "        for i, mask_position in enumerate(corrupt_mask_positions):\n",
    "            corrupt_mask_output = []\n",
    "            for pos in mask_position:\n",
    "                corrupt_mask_output.append(corrupt_outputs[i, pos, :].unsqueeze(0))\n",
    "                # |corrupt_outputs[i, pos, :].unsqueeze(0)| = (1, vocab_size)\n",
    "            corrupt_mask_output = torch.cat(corrupt_mask_output, 0)\n",
    "            # |corrupt_mask_output| = (n_masks, vocab_size)\n",
    "            corrupt_mask_outputs.append(corrupt_mask_output)\n",
    "\n",
    "        # forward for URC\n",
    "        urc_outputs = self.model(\n",
    "            urc_inputs, attention_mask=urc_attentions, output_hidden_states=True\n",
    "        )[\"hidden_states\"][-1]\n",
    "        # |urc_outputs| = (bs*3, length, hidden_size)\n",
    "        urc_logits = self.generator(urc_outputs)\n",
    "        # |urc_logits| = (bs*3, length, 3)\n",
    "        urc_cls_outputs = urc_logits[:, 0, :]\n",
    "        # |urc_cls_outputs| = (bs*3, 3)\n",
    "\n",
    "        return corrupt_mask_outputs, urc_cls_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "post_model = PostTrainModel(input_size=len(post_dataloader.dataset.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_mask_outputs, urc_cls_outputs = post_model(\n",
    "    corrupt_tokens=batch[\"mlm_inputs\"][\"corrupt_tokens\"],\n",
    "    corrupt_mask_positions=batch[\"mlm_inputs\"][\"mask_positions\"],\n",
    "    mlm_attentions=batch[\"mlm_inputs\"][\"attention_masks\"],\n",
    "    urc_inputs=batch[\"urc_inputs\"][\"input_tokens\"],\n",
    "    urc_attentions=batch[\"urc_inputs\"][\"attention_masks\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 32001])\n",
      "torch.Size([4, 32001])\n",
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "print(corrupt_mask_outputs[0].size())\n",
    "print(corrupt_mask_outputs[1].size())\n",
    "print(urc_cls_outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "NumExpr defaulting to 8 threads.\n",
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "Number of sessions: 236\n",
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization done.\n"
     ]
    }
   ],
   "source": [
    "from mrs.dataset import SessionBuilder, PostDataset, PostDatasetCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "def get_crit(pad_idx):\n",
    "    crit = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    return crit\n",
    "\n",
    "\n",
    "def get_optimizer(model, lr):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "# get dataloader\n",
    "builder = SessionBuilder(style=\"formal\")\n",
    "post_dataset = PostDataset(builder)\n",
    "\n",
    "post_dataloader = DataLoader(\n",
    "    post_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=PostDatasetCollator(\n",
    "        pad_idx=post_dataset.tokenizer.pad_token_id, max_length=99999\n",
    "    ),\n",
    ")\n",
    "\n",
    "# initialize model\n",
    "post_model = PostTrainModel(input_size=len(post_dataloader.dataset.tokenizer))\n",
    "\n",
    "training_epochs = 5\n",
    "max_grad_norm = 10\n",
    "lr = 1e-5\n",
    "n_training_steps = len(post_dataloader) * training_epochs\n",
    "n_warmup_steps = len(post_dataloader)\n",
    "crit = get_crit(pad_idx=post_dataloader.dataset.tokenizer.pad_token_id)\n",
    "optimizer = get_optimizer(post_model, lr)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=n_warmup_steps, num_training_steps=n_training_steps\n",
    ")\n",
    "\n",
    "# load model, criterion to mps\n",
    "post_model.to(\"mps:{}\".format(0))\n",
    "crit.to(\"mps:{}\".format(0))\n",
    "\n",
    "print(\"Initialization done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "**** Training Epoch: 0\n",
      "  0%|          | 0/1381 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.utils as torch_utils\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    LOGGER.info(f\"**** Training Epoch: {epoch}\")\n",
    "\n",
    "    # turn on train mode\n",
    "    post_model.train()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(post_dataloader)):\n",
    "        device = next(post_model.parameters()).device\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mlm_inputs, urc_inputs = batch[\"mlm_inputs\"], batch[\"urc_inputs\"]\n",
    "\n",
    "        mlm_corrupt_tokens, mlm_output_tokens, mlm_attention_masks = (\n",
    "            mlm_inputs[\"corrupt_tokens\"].to(device),\n",
    "            mlm_inputs[\"output_tokens\"].to(device),\n",
    "            mlm_inputs[\"attention_masks\"].to(device),\n",
    "        )\n",
    "\n",
    "        urc_input_tokens, urc_labels, urc_attention_masks = (\n",
    "            urc_inputs[\"input_tokens\"].to(device),\n",
    "            torch.cat(urc_inputs[\"labels\"], dim=0).to(device),\n",
    "            urc_inputs[\"attention_masks\"].to(device),\n",
    "        )\n",
    "\n",
    "        # forward model\n",
    "        corrupt_mask_outputs, urc_cls_outputs = post_model(\n",
    "            corrupt_tokens=mlm_corrupt_tokens,\n",
    "            corrupt_mask_positions=mlm_inputs[\"mask_positions\"],\n",
    "            mlm_attentions=mlm_attention_masks,\n",
    "            urc_inputs=urc_input_tokens,\n",
    "            urc_attentions=urc_attention_masks,\n",
    "        )\n",
    "        mlm_mask_outputs = torch.cat(corrupt_mask_outputs, dim=0)\n",
    "\n",
    "        # calculate mlm loss\n",
    "        mlm_labels = []\n",
    "        for i, mask_position in enumerate(mlm_inputs[\"mask_positions\"]):\n",
    "            org_token_indices = []\n",
    "            for pos in mask_position:\n",
    "                org_token_indices.append(mlm_output_tokens[i, pos].item())\n",
    "            mlm_labels.extend(org_token_indices)\n",
    "        mlm_labels = torch.tensor(mlm_labels).long().to(device)\n",
    "\n",
    "        mlm_loss = crit(mlm_mask_outputs, mlm_labels)\n",
    "\n",
    "        # calculate urc loss\n",
    "        urc_loss = crit(urc_cls_outputs, urc_labels)\n",
    "\n",
    "        # sum of mlm loss and urc loss\n",
    "        total_loss = mlm_loss + urc_loss\n",
    "\n",
    "        # backward loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        # clip gradient\n",
    "        torch_utils.clip_grad_norm_(post_model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
