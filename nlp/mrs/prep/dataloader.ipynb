{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Project     : mrs - multi-turn response selection\n",
    "# Created By  : Eungis\n",
    "# Team        : AI Engineering\n",
    "# Created Date: 2023-11-30\n",
    "# Updated Date: 2023-12-22\n",
    "# Purpose     : Make data_loader for loading data\n",
    "# version     : 0.0.1\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from importlib import reload  # Not needed in Python 2\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(\n",
    "    format=\"%(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "DATA_ROOT = \"../data/\"\n",
    "logger = logging.getLogger()\n",
    "data = pd.read_csv(DATA_ROOT + \"smilestyle_dataset.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns: ['formal', 'informal', 'android', 'azae', 'chat', 'choding', 'emoticon', 'enfp', 'gentle', 'halbae', 'halmae', 'joongding', 'king', 'naruto', 'seonbi', 'sosim', 'translator']\n",
      "Number of groups: 236\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "cols = data.columns.tolist()\n",
    "logger.debug(f\"Columns: {cols}\")\n",
    "# use formal conversational data\n",
    "data = data[[\"formal\"]]\n",
    "\n",
    "data[\"group\"] = data[\"formal\"].isnull().cumsum()\n",
    "n_sessions = data[\"group\"].iat[-1] + 1\n",
    "logger.debug(f\"Number of groups: {n_sessions}\")\n",
    "\n",
    "# split data into sessions\n",
    "sessions: List[List[str]] = []\n",
    "groups = data.groupby(\"group\", as_index=False, group_keys=False)\n",
    "\n",
    "for i, group in groups:\n",
    "    session = group.dropna()[\"formal\"].tolist()\n",
    "    sessions += [session]\n",
    "\n",
    "assert n_sessions == len(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요. 저는 고양이 6마리 키워요.',\n",
       " '고양이를 6마리나요? 키우는거 안 힘드세요?',\n",
       " '제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.',\n",
       " '가장 나이가 많은 고양이가 어떻게 돼요?',\n",
       " '여섯 살입니다. 갈색 고양이에요.',\n",
       " '그럼 가장 어린 고양이가 어떻게 돼요?',\n",
       " '한 살입니다. 작년에 분양 받았어요.',\n",
       " '그럼 고양이들끼리 안 싸우나요?',\n",
       " '저희 일곱은 다같이 한 가족입니다. 싸우는 일은 없어요.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "Special tokens: {'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "EOS token & SEP token: [SEP] / [SEP]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "logger.debug(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "logger.debug(f\"EOS token & SEP token: {tokenizer.eos_token} / {tokenizer.sep_token}\")\n",
    "special_tokens = {\"sep_token\": \"<SEP>\"}\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens map: {'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '<SEP>', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "[SEP]: 2\n",
      "<SEP>: 32000\n",
      "[MASK]: 4\n"
     ]
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"\"\"Special tokens map: {tokenizer.special_tokens_map}\n",
    "{tokenizer.eos_token}: {tokenizer.eos_token_id}\n",
    "{tokenizer.sep_token}: {tokenizer.sep_token_id}\n",
    "{tokenizer.mask_token}: {tokenizer.mask_token_id}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이가 어떻게 돼요? <SEP> 여섯 살입니다. 갈색 고양이에요. <SEP> 그럼 가장 어린 고양이가 어떻게 돼요? <SEP> 한 살입니다. 작년에 분양 받았어요. <SEP> 그럼 고양이들끼리 안 싸우나요? <SEP> 저희 일곱은 다같이 한 가족입니다. 싸우는 일은 없어요.\n",
      "안녕하세요. 저는 고양이 6마리 [MASK]요. <SEP> 고양이 [MASK] 6마리나요? 키우는 [MASK] 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 [MASK]진 않 [MASK]요. <SEP> 가장 나이가 [MASK]은 고양이가 어떻게 돼요? <SEP> 여섯 살입니다. 갈색 고양이에 [MASK]. <SEP> [MASK] 가장 어린 고양이가 어떻게 돼요? <SEP> 한 살입니다 [MASK] 작년에 분양 받았어요. <SEP> 그럼 고양이들끼리 안 싸우나 [MASK]? <SEP> 저희 일곱은 다같이 한 가족입니다 [MASK] 싸우는 일은 [MASK]어요.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "session = sessions[0]\n",
    "mask_ratio = 0.15\n",
    "corrupt_tokens = []\n",
    "output_tokens = []\n",
    "for i, utt in enumerate(session):\n",
    "    original_token = tokenizer.encode(utt, add_special_tokens=False)\n",
    "    n_mask = int(len(original_token) * mask_ratio)\n",
    "    mask_positions = random.sample([x for x in range(len(original_token))], n_mask)\n",
    "    corrupt_token = []\n",
    "    for pos in range(len(original_token)):\n",
    "        if pos in mask_positions:\n",
    "            corrupt_token.append(tokenizer.mask_token_id)\n",
    "        else:\n",
    "            corrupt_token.append(original_token[pos])\n",
    "    if i == len(session) - 1:\n",
    "        output_tokens.extend(original_token)\n",
    "        corrupt_tokens.extend(corrupt_token)\n",
    "    else:\n",
    "        output_tokens.extend(original_token + [tokenizer.sep_token_id])\n",
    "        corrupt_tokens.extend(corrupt_token + [tokenizer.sep_token_id])\n",
    "\n",
    "logger.debug(tokenizer.decode(output_tokens))\n",
    "logger.debug(tokenizer.decode(corrupt_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2762\n",
      "['안녕하세요. 저는 고양이 6마리 키워요.', '고양이를 6마리나요? 키우는거 안 힘드세요?', '제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.', '가장 나이가 많은 고양이가 어떻게 돼요?']\n",
      "['고양이를 6마리나요? 키우는거 안 힘드세요?', '제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.', '가장 나이가 많은 고양이가 어떻게 돼요?', '여섯 살입니다. 갈색 고양이에요.']\n"
     ]
    }
   ],
   "source": [
    "# construct short sessions\n",
    "k = 4\n",
    "short_sessions = []\n",
    "for session in sessions:\n",
    "    for i in range(len(session) - k + 1):\n",
    "        short_sessions.append(session[i : i + k])\n",
    "logger.debug(len(short_sessions))\n",
    "logger.info(short_sessions[0])\n",
    "logger.info(short_sessions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of negative samples: 3430\n"
     ]
    }
   ],
   "source": [
    "# construct negative response candidates\n",
    "import random\n",
    "\n",
    "all_utts = set()\n",
    "for session in sessions:\n",
    "    for utt in session:\n",
    "        all_utts.add(utt)\n",
    "all_utts = list(all_utts)\n",
    "logger.info(f\"Number of negative samples: {len(all_utts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 아니요, 그렇게는 절대로 살기 싫습니다.\n",
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 안녕하세요. 저는 고양이 6마리 키워요.\n"
     ]
    }
   ],
   "source": [
    "session = short_sessions[0]\n",
    "urc_tokens = []\n",
    "context_utts = []\n",
    "\n",
    "for i in range(len(session)):\n",
    "    utt = session[i]\n",
    "    original_token = tokenizer.encode(utt, add_special_tokens=False)\n",
    "    if i == len(session) - 1:\n",
    "        positive_tokens = urc_tokens + original_token\n",
    "        while True:\n",
    "            random_neg_response = random.choice(all_utts)\n",
    "            if random_neg_response not in context_utts:\n",
    "                break\n",
    "        # random negative response\n",
    "        random_neg_response_token = tokenizer.encode(\n",
    "            random_neg_response, add_special_tokens=False\n",
    "        )\n",
    "        random_tokens = urc_tokens + random_neg_response_token\n",
    "\n",
    "        # context negative response\n",
    "        context_neg_response = random.choice(context_utts)\n",
    "        context_neg_response_token = tokenizer.encode(\n",
    "            context_neg_response, add_special_tokens=False\n",
    "        )\n",
    "        context_neg_tokens = urc_tokens + context_neg_response_token\n",
    "    else:\n",
    "        urc_tokens += original_token + [tokenizer.sep_token_id]\n",
    "    context_utts.append(utt)\n",
    "\n",
    "logger.debug(tokenizer.decode(positive_tokens))\n",
    "logger.debug(tokenizer.decode(random_tokens))\n",
    "logger.debug(tokenizer.decode(context_neg_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
