{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Project     : mrs - multi-turn response selection\n",
    "# Created By  : Eungis\n",
    "# Team        : AI Engineering\n",
    "# Created Date: 2023-11-30\n",
    "# Updated Date: 2024-01-03\n",
    "# Purpose     : Make data_loader for loading data\n",
    "# version     : 0.0.1\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from importlib import reload  # Not needed in Python 2\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(\n",
    "    format=\"%(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "DATA_ROOT = \"../data/\"\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Session:\n",
    "    conv: List[str]\n",
    "    \"\"\"Conversation: List of utterences\"\"\"\n",
    "\n",
    "\n",
    "class SessionBuilder:\n",
    "    def __init__(self, style: str):\n",
    "        self.style = style\n",
    "        self.logger = self._set_logger()\n",
    "\n",
    "    def _set_logger(self):\n",
    "        logging.basicConfig(\n",
    "            format=\"%(message)s\",\n",
    "            level=logging.DEBUG,\n",
    "        )\n",
    "        logger = logging.getLogger()\n",
    "        return logger\n",
    "\n",
    "    def build_sessions(self, data_path: str) -> List[List[str]]:\n",
    "        # data to load must be separated with `tab`\n",
    "        data = pd.read_csv(data_path, sep=\"\\t\")\n",
    "        styles = data.columns.tolist()\n",
    "        if self.style not in styles:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported style. Style must be one of {styles}.\\nInput: {self.style}\"\n",
    "            )\n",
    "\n",
    "        # use specified style conversational data\n",
    "        data = data[[self.style]]\n",
    "        data[\"group\"] = data[self.style].isnull().cumsum()\n",
    "        n_sessions = data[\"group\"].iat[-1] + 1\n",
    "        logger.debug(f\"Number of sessions: {n_sessions}\")\n",
    "\n",
    "        # split data into sessions\n",
    "        sessions: List[Session] = []\n",
    "        groups = data.groupby(\"group\", as_index=False, group_keys=False)\n",
    "\n",
    "        for i, group in groups:\n",
    "            session = group.dropna()[self.style].tolist()\n",
    "            sessions += [Session(conv=session)]\n",
    "\n",
    "        assert n_sessions == len(sessions)\n",
    "        return sessions\n",
    "\n",
    "    def build_short_sessions(\n",
    "        self, sessions: List[Session], ctx_len: int = 4\n",
    "    ) -> List[Session]:\n",
    "        short_sessions = []\n",
    "        for session in sessions:\n",
    "            for i in range(len(session.conv) - ctx_len + 1):\n",
    "                short_sessions.append(Session(conv=session.conv[i : i + ctx_len]))\n",
    "        return short_sessions\n",
    "\n",
    "    def get_utterances(self, sessions: List[Session]):\n",
    "        all_utts = set()\n",
    "        for session in sessions:\n",
    "            for utt in session.conv:\n",
    "                all_utts.add(utt)\n",
    "        return list(all_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of sessions: 236\n",
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "Special tokens: {'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "EOS token & SEP token: [SEP] / [SEP]\n",
      "Special tokens map: {'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '<SEP>', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "[SEP]: 2\n",
      "<SEP>: 32000\n",
      "[MASK]: 4\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "builder = SessionBuilder(style=\"formal\")\n",
    "sessions = builder.build_sessions(data_path=DATA_ROOT + \"smilestyle_dataset.tsv\")\n",
    "short_sessions = builder.build_short_sessions(sessions, ctx_len=4)\n",
    "utts = builder.get_utterances(sessions)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "logger.debug(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "logger.debug(f\"EOS token & SEP token: {tokenizer.eos_token} / {tokenizer.sep_token}\")\n",
    "special_tokens = {\"sep_token\": \"<SEP>\"}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "logger.info(\n",
    "    f\"\"\"Special tokens map: {tokenizer.special_tokens_map}\n",
    "{tokenizer.eos_token}: {tokenizer.eos_token_id}\n",
    "{tokenizer.sep_token}: {tokenizer.sep_token_id}\n",
    "{tokenizer.mask_token}: {tokenizer.mask_token_id}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(self, sessions):\n",
    "#     \"\"\"\n",
    "#     input:\n",
    "#         data: [(session1), (session2), ... ]\n",
    "#     return:\n",
    "#         batch_corrupt_tokens: (B, L) padded\n",
    "#         batch_output_tokens: (B, L) padded\n",
    "#         batch_corrupt_mask_positions: list\n",
    "#         batch_urc_inputs: (B, L) padded\n",
    "#         batch_urc_labels: (B)\n",
    "#         batch_mlm_attentions\n",
    "#         batch_urc_attentions\n",
    "\n",
    "#     batch가 3\n",
    "#     MLM = 3개의 입력데이터 (입력데이터별로 길이가 다름)\n",
    "#     URC = 9개의 입력데이터 (ctx는 길이가 다름, response candidate도 길이가 다름)\n",
    "#     \"\"\"\n",
    "#     (\n",
    "#         batch_corrupt_tokens,\n",
    "#         batch_output_tokens,\n",
    "#         batch_corrupt_mask_positions,\n",
    "#         batch_urc_inputs,\n",
    "#         batch_urc_labels,\n",
    "#     ) = ([], [], [], [], [])\n",
    "#     batch_mlm_attentions, batch_urc_attentions = [], []\n",
    "#     # MLM, URC 입력에 대해서 가장 긴 입력 길이를 찾기\n",
    "#     corrupt_max_len, urc_max_len = 0, 0\n",
    "#     for session in sessions:\n",
    "#         (\n",
    "#             corrupt_tokens,\n",
    "#             output_tokens,\n",
    "#             corrupt_mask_positions,\n",
    "#             urc_inputs,\n",
    "#             urc_labels,\n",
    "#         ) = session\n",
    "#         if len(corrupt_tokens) > corrupt_max_len:\n",
    "#             corrupt_max_len = len(corrupt_tokens)\n",
    "#         positive_tokens, random_tokens, ctx_neg_tokens = urc_inputs\n",
    "#         if (\n",
    "#             max([len(positive_tokens), len(random_tokens), len(ctx_neg_tokens)])\n",
    "#             > urc_max_len\n",
    "#         ):\n",
    "#             urc_max_len = max(\n",
    "#                 [len(positive_tokens), len(random_tokens), len(ctx_neg_tokens)]\n",
    "#             )\n",
    "\n",
    "#     ## padding 토큰을 추가하는 부분\n",
    "#     for session in sessions:\n",
    "#         (\n",
    "#             corrupt_tokens,\n",
    "#             output_tokens,\n",
    "#             corrupt_mask_positions,\n",
    "#             urc_inputs,\n",
    "#             urc_labels,\n",
    "#         ) = session\n",
    "#         \"\"\" mlm 입력 \"\"\"\n",
    "#         batch_corrupt_tokens.append(\n",
    "#             corrupt_tokens\n",
    "#             + [\n",
    "#                 self.tokenizer.pad_token_id\n",
    "#                 for _ in range(corrupt_max_len - len(corrupt_tokens))\n",
    "#             ]\n",
    "#         )\n",
    "#         batch_mlm_attentions.append(\n",
    "#             [1 for _ in range(len(corrupt_tokens))]\n",
    "#             + [0 for _ in range(corrupt_max_len - len(corrupt_tokens))]\n",
    "#         )\n",
    "\n",
    "#         \"\"\" mlm 출력 \"\"\"\n",
    "#         batch_output_tokens.append(\n",
    "#             output_tokens\n",
    "#             + [\n",
    "#                 self.tokenizer.pad_token_id\n",
    "#                 for _ in range(corrupt_max_len - len(corrupt_tokens))\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#         \"\"\" mlm 레이블 \"\"\"\n",
    "#         batch_corrupt_mask_positions.append(corrupt_mask_positions)\n",
    "\n",
    "#         \"\"\" urc 입력 \"\"\"\n",
    "#         # positive_tokens, random_tokens, ctx_neg_tokens = urc_inputs\n",
    "#         for urc_input in urc_inputs:\n",
    "#             batch_urc_inputs.append(\n",
    "#                 urc_input\n",
    "#                 + [\n",
    "#                     self.tokenizer.pad_token_id\n",
    "#                     for _ in range(urc_max_len - len(urc_input))\n",
    "#                 ]\n",
    "#             )\n",
    "#             batch_urc_attentions.append(\n",
    "#                 [1 for _ in range(len(urc_input))]\n",
    "#                 + [0 for _ in range(urc_max_len - len(urc_input))]\n",
    "#             )\n",
    "\n",
    "#         \"\"\" urc 레이블 \"\"\"\n",
    "#         batch_urc_labels += urc_labels\n",
    "#     return (\n",
    "#         torch.tensor(batch_corrupt_tokens),\n",
    "#         torch.tensor(batch_output_tokens),\n",
    "#         batch_corrupt_mask_positions,\n",
    "#         torch.tensor(batch_urc_inputs),\n",
    "#         torch.tensor(batch_urc_labels),\n",
    "#         torch.tensor(batch_mlm_attentions),\n",
    "#         torch.tensor(batch_urc_attentions),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from importlib import reload  # Not needed in Python 2\n",
    "from typing import List, Dict\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"klue/roberta-base\"\n",
    "DATA_ROOT = \"../data/\"\n",
    "DATA_PATH = DATA_ROOT + \"smilestyle_dataset.tsv\"\n",
    "\n",
    "\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, builder: SessionBuilder):\n",
    "        # set logger\n",
    "        self.logger = self._set_logger()\n",
    "\n",
    "        # set tokenizer\n",
    "        self.tokenizer = self._set_tokenizer()\n",
    "\n",
    "        # build sessions\n",
    "        self.sessions = builder.build_sessions(data_path=DATA_PATH)\n",
    "        self.short_sessions = builder.build_short_sessions(self.sessions, ctx_len=4)\n",
    "        self.utts = builder.get_utterances(self.sessions)\n",
    "\n",
    "    def _set_logger(self):\n",
    "        logging.basicConfig(\n",
    "            format=\"%(message)s\",\n",
    "            level=logging.DEBUG,\n",
    "        )\n",
    "        logger = logging.getLogger()\n",
    "        return logger\n",
    "\n",
    "    def _set_tokenizer(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        special_tokens = {\"sep_token\": \"<SEP>\"}\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        return tokenizer\n",
    "\n",
    "    def _mask_tokens(self, tokens: List, ratio: float = 0.15) -> List:\n",
    "        tokens = np.array(tokens)\n",
    "        n_mask = int(len(tokens) * 0.15)\n",
    "        # n_mask = math.ceil(len(tokens) * 0.15)\n",
    "        mask_pos = random.sample(range(len(tokens)), n_mask)\n",
    "\n",
    "        # fancy indexing\n",
    "        tokens[mask_pos] = self.tokenizer.mask_token_id\n",
    "        tokens = tokens.tolist()\n",
    "        return tokens\n",
    "\n",
    "    def _get_mask_positions(self, tokens: List) -> List:\n",
    "        tokens = np.array(tokens)\n",
    "        mask_positions = np.where(tokens == self.tokenizer.mask_token_id)[0].tolist()\n",
    "        return mask_positions\n",
    "\n",
    "    def construct_mlm_inputs(self, short_session: Session) -> Dict[str, list]:\n",
    "        corrupt_tokens = []\n",
    "        output_tokens = []\n",
    "\n",
    "        for i, utt in enumerate(short_session.conv):\n",
    "            tokens = self.tokenizer.encode(utt, add_special_tokens=False)\n",
    "            masked_tokens = self._mask_tokens(tokens)\n",
    "\n",
    "            if i == len(short_session.conv) - 1:\n",
    "                output_tokens.extend(tokens)\n",
    "                corrupt_tokens.extend(masked_tokens)\n",
    "            else:\n",
    "                output_tokens.extend(tokens + [self.tokenizer.sep_token_id])\n",
    "                corrupt_tokens.extend(masked_tokens + [self.tokenizer.sep_token_id])\n",
    "        corrupt_mask_positions = self._get_mask_positions(corrupt_tokens)\n",
    "        return_value = {\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"corrupt_tokens\": corrupt_tokens,\n",
    "            \"corrupt_mask_positions\": corrupt_mask_positions,\n",
    "        }\n",
    "\n",
    "        return return_value\n",
    "\n",
    "    def construct_urc_inputs(self, short_session: Session) -> Dict[str, List]:\n",
    "        urc_tokens = []\n",
    "        ctx_utts = []\n",
    "\n",
    "        for i in range(len(short_session.conv)):\n",
    "            utt = short_session.conv[i]\n",
    "            tokens = self.tokenizer.encode(utt, add_special_tokens=False)\n",
    "\n",
    "            if i == len(short_session.conv) - 1:\n",
    "                urc_tokens += [self.tokenizer.eos_token_id]\n",
    "                positive_tokens = [self.tokenizer.cls_token_id] + urc_tokens + tokens\n",
    "                while True:\n",
    "                    random_neg_response = random.choice(self.utts)\n",
    "                    if random_neg_response not in ctx_utts:\n",
    "                        break\n",
    "                random_neg_response_token = self.tokenizer.encode(\n",
    "                    random_neg_response, add_special_tokens=False\n",
    "                )\n",
    "                random_tokens = (\n",
    "                    [self.tokenizer.cls_token_id]\n",
    "                    + urc_tokens\n",
    "                    + random_neg_response_token\n",
    "                )\n",
    "                ctx_neg_response = random.choice(ctx_utts)\n",
    "                ctx_neg_response_token = self.tokenizer.encode(\n",
    "                    ctx_neg_response, add_special_tokens=False\n",
    "                )\n",
    "                ctx_neg_tokens = (\n",
    "                    [self.tokenizer.cls_token_id] + urc_tokens + ctx_neg_response_token\n",
    "                )\n",
    "            else:\n",
    "                urc_tokens += tokens + [self.tokenizer.sep_token_id]\n",
    "\n",
    "            ctx_utts.append(utt)\n",
    "\n",
    "        return_value = {\n",
    "            \"positive_tokens\": positive_tokens,\n",
    "            \"random_negative_tokens\": random_tokens,\n",
    "            \"context_negative_tokens\": ctx_neg_tokens,\n",
    "            \"urc_labels\": [0, 1, 2],\n",
    "        }\n",
    "        return return_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.short_sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ---- input data for MLM ---- #\n",
    "        short_session = self.short_sessions[idx]\n",
    "        mlm_input = self.construct_mlm_inputs(short_session)\n",
    "\n",
    "        # ---- intput data for utterance relevance classification ---- #\n",
    "        urc_input = self.construct_urc_inputs(short_session)\n",
    "\n",
    "        return_value = dict()\n",
    "        return_value[\"mlm_input\"] = mlm_input\n",
    "        return_value[\"urc_input\"] = urc_input\n",
    "\n",
    "        return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "Number of sessions: 236\n"
     ]
    }
   ],
   "source": [
    "builder = SessionBuilder(style=\"formal\")\n",
    "post_dataset = PostDataset(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "안녕하세요. 저 [MASK] 고양이 6마리 키워요. <SEP> 고양이 [MASK] 6마리나요? 키우 [MASK]거 안 힘드세요? <SEP> [MASK]가 [MASK] 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이 [MASK] 어떻게 돼요?\n",
      "[5, 14, 21, 28, 30, 49]\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 네, 가족이랑 여행으로 왔습니다.\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 안녕하세요. 저는 고양이 6마리 키워요.\n"
     ]
    }
   ],
   "source": [
    "sample = post_dataset[0]\n",
    "print(post_dataset.tokenizer.decode(sample[\"mlm_input\"][\"output_tokens\"]))\n",
    "print(post_dataset.tokenizer.decode(sample[\"mlm_input\"][\"corrupt_tokens\"]))\n",
    "print(sample[\"mlm_input\"][\"corrupt_mask_positions\"])\n",
    "print(post_dataset.tokenizer.decode(sample[\"urc_input\"][\"positive_tokens\"]))\n",
    "print(post_dataset.tokenizer.decode(sample[\"urc_input\"][\"random_negative_tokens\"]))\n",
    "print(post_dataset.tokenizer.decode(sample[\"urc_input\"][\"context_negative_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "Columns: ['formal', 'informal', 'android', 'azae', 'chat', 'choding', 'emoticon', 'enfp', 'gentle', 'halbae', 'halmae', 'joongding', 'king', 'naruto', 'seonbi', 'sosim', 'translator']\n",
      "Number of groups: 236\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "post_dataloader = DataLoader(\n",
    "    post_dataset, batch_size=2, shuffle=True, collate_fn=post_dataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataLoader\n",
    "(\n",
    "    batch_corrupt_tokens,\n",
    "    batch_output_tokens,\n",
    "    batch_corrupt_mask_positions,\n",
    "    batch_urc_inputs,\n",
    "    batch_urc_labels,\n",
    "    batch_mlm_attentions,\n",
    "    batch_urc_attentions,\n",
    ") = next(iter(post_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] csr가 뭔지는 모르겠지만, 도어 대시라는 회사는 알고 있습니다. 큰 회사였잖아요. <SEP> csr는 고객 서비스 담당자의 약자입니다. 해당 부서가 축소되면서 저도 잘렸습니다. <SEP> 그러면 지금은 뭘 하는 중이세요? <SEP> [SEP] 경쟁 회사에 취직하고 나서는, 일 뿐만 아니라 취미활동도 하고 있습니다. 퇴근하고 나면 근처에 있는 종합체육시설에서 수영을 해요.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_dataset.tokenizer.decode(batch_urc_inputs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
