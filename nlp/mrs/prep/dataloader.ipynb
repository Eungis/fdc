{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Project     : mrs - multi-turn response selection\n",
    "# Created By  : Eungis\n",
    "# Team        : AI Engineering\n",
    "# Created Date: 2023-11-30\n",
    "# Updated Date: 2023-12-24\n",
    "# Purpose     : Make data_loader for loading data\n",
    "# version     : 0.0.1\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from importlib import reload  # Not needed in Python 2\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(\n",
    "    format=\"%(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "DATA_ROOT = \"../data/\"\n",
    "logger = logging.getLogger()\n",
    "data = pd.read_csv(DATA_ROOT + \"smilestyle_dataset.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns: ['formal', 'informal', 'android', 'azae', 'chat', 'choding', 'emoticon', 'enfp', 'gentle', 'halbae', 'halmae', 'joongding', 'king', 'naruto', 'seonbi', 'sosim', 'translator']\n",
      "Number of groups: 236\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "cols = data.columns.tolist()\n",
    "logger.debug(f\"Columns: {cols}\")\n",
    "# use formal conversational data\n",
    "data = data[[\"formal\"]]\n",
    "\n",
    "data[\"group\"] = data[\"formal\"].isnull().cumsum()\n",
    "n_sessions = data[\"group\"].iat[-1] + 1\n",
    "logger.debug(f\"Number of groups: {n_sessions}\")\n",
    "\n",
    "# split data into sessions\n",
    "sessions: List[List[str]] = []\n",
    "groups = data.groupby(\"group\", as_index=False, group_keys=False)\n",
    "\n",
    "for i, group in groups:\n",
    "    session = group.dropna()[\"formal\"].tolist()\n",
    "    sessions += [session]\n",
    "\n",
    "assert n_sessions == len(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요. 저는 고양이 6마리 키워요.',\n",
       " '고양이를 6마리나요? 키우는거 안 힘드세요?',\n",
       " '제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.',\n",
       " '가장 나이가 많은 고양이가 어떻게 돼요?',\n",
       " '여섯 살입니다. 갈색 고양이에요.',\n",
       " '그럼 가장 어린 고양이가 어떻게 돼요?',\n",
       " '한 살입니다. 작년에 분양 받았어요.',\n",
       " '그럼 고양이들끼리 안 싸우나요?',\n",
       " '저희 일곱은 다같이 한 가족입니다. 싸우는 일은 없어요.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "Special tokens: {'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "EOS token & SEP token: [SEP] / [SEP]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "logger.debug(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "logger.debug(f\"EOS token & SEP token: {tokenizer.eos_token} / {tokenizer.sep_token}\")\n",
    "special_tokens = {\"sep_token\": \"<SEP>\"}\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens map: {'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '<SEP>', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "[SEP]: 2\n",
      "<SEP>: 32000\n",
      "[MASK]: 4\n"
     ]
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"\"\"Special tokens map: {tokenizer.special_tokens_map}\n",
    "{tokenizer.eos_token}: {tokenizer.eos_token_id}\n",
    "{tokenizer.sep_token}: {tokenizer.sep_token_id}\n",
    "{tokenizer.mask_token}: {tokenizer.mask_token_id}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이가 어떻게 돼요? <SEP> 여섯 살입니다. 갈색 고양이에요. <SEP> 그럼 가장 어린 고양이가 어떻게 돼요? <SEP> 한 살입니다. 작년에 분양 받았어요. <SEP> 그럼 고양이들끼리 안 싸우나요? <SEP> 저희 일곱은 다같이 한 가족입니다. 싸우는 일은 없어요.\n",
      "안녕하세요. 저는 고양이 6마리 [MASK]요. <SEP> 고양이 [MASK] 6마리나요? 키우는 [MASK] 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 [MASK]진 않 [MASK]요. <SEP> 가장 나이가 [MASK]은 고양이가 어떻게 돼요? <SEP> 여섯 살입니다. 갈색 고양이에 [MASK]. <SEP> [MASK] 가장 어린 고양이가 어떻게 돼요? <SEP> 한 살입니다 [MASK] 작년에 분양 받았어요. <SEP> 그럼 고양이들끼리 안 싸우나 [MASK]? <SEP> 저희 일곱은 다같이 한 가족입니다 [MASK] 싸우는 일은 [MASK]어요.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "session = sessions[0]\n",
    "mask_ratio = 0.15\n",
    "corrupt_tokens = []\n",
    "output_tokens = []\n",
    "for i, utt in enumerate(session):\n",
    "    original_token = tokenizer.encode(utt, add_special_tokens=False)\n",
    "    n_mask = int(len(original_token) * mask_ratio)\n",
    "    mask_positions = random.sample([x for x in range(len(original_token))], n_mask)\n",
    "    corrupt_token = []\n",
    "    for pos in range(len(original_token)):\n",
    "        if pos in mask_positions:\n",
    "            corrupt_token.append(tokenizer.mask_token_id)\n",
    "        else:\n",
    "            corrupt_token.append(original_token[pos])\n",
    "    if i == len(session) - 1:\n",
    "        output_tokens.extend(original_token)\n",
    "        corrupt_tokens.extend(corrupt_token)\n",
    "    else:\n",
    "        output_tokens.extend(original_token + [tokenizer.sep_token_id])\n",
    "        corrupt_tokens.extend(corrupt_token + [tokenizer.sep_token_id])\n",
    "\n",
    "logger.debug(tokenizer.decode(output_tokens))\n",
    "logger.debug(tokenizer.decode(corrupt_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2762\n",
      "['안녕하세요. 저는 고양이 6마리 키워요.', '고양이를 6마리나요? 키우는거 안 힘드세요?', '제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.', '가장 나이가 많은 고양이가 어떻게 돼요?']\n",
      "['고양이를 6마리나요? 키우는거 안 힘드세요?', '제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.', '가장 나이가 많은 고양이가 어떻게 돼요?', '여섯 살입니다. 갈색 고양이에요.']\n"
     ]
    }
   ],
   "source": [
    "# construct short sessions\n",
    "k = 4\n",
    "short_sessions = []\n",
    "for session in sessions:\n",
    "    for i in range(len(session) - k + 1):\n",
    "        short_sessions.append(session[i : i + k])\n",
    "logger.debug(len(short_sessions))\n",
    "logger.info(short_sessions[0])\n",
    "logger.info(short_sessions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of negative samples: 3430\n"
     ]
    }
   ],
   "source": [
    "# construct negative response candidates\n",
    "import random\n",
    "\n",
    "all_utts = set()\n",
    "for session in sessions:\n",
    "    for utt in session:\n",
    "        all_utts.add(utt)\n",
    "all_utts = list(all_utts)\n",
    "logger.info(f\"Number of negative samples: {len(all_utts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 아니요, 그렇게는 절대로 살기 싫습니다.\n",
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 안녕하세요. 저는 고양이 6마리 키워요.\n"
     ]
    }
   ],
   "source": [
    "session = short_sessions[0]\n",
    "urc_tokens = []\n",
    "context_utts = []\n",
    "\n",
    "for i in range(len(session)):\n",
    "    utt = session[i]\n",
    "    original_token = tokenizer.encode(utt, add_special_tokens=False)\n",
    "    if i == len(session) - 1:\n",
    "        positive_tokens = urc_tokens + original_token\n",
    "        while True:\n",
    "            random_neg_response = random.choice(all_utts)\n",
    "            if random_neg_response not in context_utts:\n",
    "                break\n",
    "        # random negative response\n",
    "        random_neg_response_token = tokenizer.encode(\n",
    "            random_neg_response, add_special_tokens=False\n",
    "        )\n",
    "        random_tokens = urc_tokens + random_neg_response_token\n",
    "\n",
    "        # context negative response\n",
    "        context_neg_response = random.choice(context_utts)\n",
    "        context_neg_response_token = tokenizer.encode(\n",
    "            context_neg_response, add_special_tokens=False\n",
    "        )\n",
    "        context_neg_tokens = urc_tokens + context_neg_response_token\n",
    "    else:\n",
    "        urc_tokens += original_token + [tokenizer.sep_token_id]\n",
    "    context_utts.append(utt)\n",
    "\n",
    "logger.debug(tokenizer.decode(positive_tokens))\n",
    "logger.debug(tokenizer.decode(random_tokens))\n",
    "logger.debug(tokenizer.decode(context_neg_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import logging\n",
    "from importlib import reload  # Not needed in Python 2\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"klue/roberta-base\"\n",
    "DATA_ROOT = \"../data/\"\n",
    "DATA_PATH = DATA_ROOT + \"smilestyle_dataset.tsv\"\n",
    "\n",
    "\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, data_path: str, ctx_len: int = 4):\n",
    "        self.logger = self._set_logger()\n",
    "\n",
    "        # set tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        special_tokens = {\"sep_token\": \"<SEP>\"}\n",
    "        self.tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "        # construct sessions\n",
    "        sessions = self._construct_sessions(data_path)\n",
    "\n",
    "        # construct short sessions\n",
    "        self.short_sessions = self._construct_short_sessions(sessions, ctx_len=ctx_len)\n",
    "\n",
    "        # get all utterances\n",
    "        self.all_utts = self._get_utterances(sessions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.short_sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Input data for MLM\n",
    "        session = self.short_sessions[idx]\n",
    "        mask_ratio = 0.15\n",
    "        self.corrupt_tokens = []\n",
    "        self.output_tokens = []\n",
    "        for i, utt in enumerate(session):\n",
    "            original_token = self.tokenizer.encode(utt, add_special_tokens=False)\n",
    "\n",
    "            mask_num = int(len(original_token) * mask_ratio)\n",
    "            mask_positions = random.sample(\n",
    "                [x for x in range(len(original_token))], mask_num\n",
    "            )\n",
    "            corrupt_token = []\n",
    "            for pos in range(len(original_token)):\n",
    "                if pos in mask_positions:\n",
    "                    corrupt_token.append(self.tokenizer.mask_token_id)\n",
    "                else:\n",
    "                    corrupt_token.append(original_token[pos])\n",
    "\n",
    "            if i == len(session) - 1:\n",
    "                self.output_tokens += original_token\n",
    "                self.corrupt_tokens += corrupt_token\n",
    "            else:\n",
    "                self.output_tokens += original_token + [self.tokenizer.sep_token_id]\n",
    "                self.corrupt_tokens += corrupt_token + [self.tokenizer.sep_token_id]\n",
    "\n",
    "        # Label for loss\n",
    "        self.corrupt_mask_positions = []\n",
    "        for pos in range(len(self.corrupt_tokens)):\n",
    "            if self.corrupt_tokens[pos] == self.tokenizer.mask_token_id:\n",
    "                self.corrupt_mask_positions.append(pos)\n",
    "\n",
    "        # URC\n",
    "        urc_tokens = []\n",
    "        context_utts = []\n",
    "        for i in range(len(session)):\n",
    "            utt = session[i]\n",
    "            original_token = self.tokenizer.encode(utt, add_special_tokens=False)\n",
    "            if i == len(session) - 1:\n",
    "                urc_tokens += [self.tokenizer.eos_token_id]\n",
    "                self.positive_tokens = (\n",
    "                    [self.tokenizer.cls_token_id] + urc_tokens + original_token\n",
    "                )\n",
    "                while True:\n",
    "                    random_neg_response = random.choice(self.all_utts)\n",
    "                    if random_neg_response not in context_utts:\n",
    "                        break\n",
    "                random_neg_response_token = self.tokenizer.encode(\n",
    "                    random_neg_response, add_special_tokens=False\n",
    "                )\n",
    "                self.random_tokens = (\n",
    "                    [self.tokenizer.cls_token_id]\n",
    "                    + urc_tokens\n",
    "                    + random_neg_response_token\n",
    "                )\n",
    "                context_neg_response = random.choice(context_utts)\n",
    "                context_neg_response_token = self.tokenizer.encode(\n",
    "                    context_neg_response, add_special_tokens=False\n",
    "                )\n",
    "                self.context_neg_tokens = (\n",
    "                    [self.tokenizer.cls_token_id]\n",
    "                    + urc_tokens\n",
    "                    + context_neg_response_token\n",
    "                )\n",
    "            else:\n",
    "                urc_tokens += original_token + [self.tokenizer.sep_token_id]\n",
    "            context_utts.append(utt)\n",
    "\n",
    "        return (\n",
    "            self.corrupt_tokens,\n",
    "            self.output_tokens,\n",
    "            self.corrupt_mask_positions,\n",
    "            [self.positive_tokens, self.random_tokens, self.context_neg_tokens],\n",
    "            [0, 1, 2],\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, sessions):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            data: [(session1), (session2), ... ]\n",
    "        return:\n",
    "            batch_corrupt_tokens: (B, L) padded\n",
    "            batch_output_tokens: (B, L) padded\n",
    "            batch_corrupt_mask_positions: list\n",
    "            batch_urc_inputs: (B, L) padded\n",
    "            batch_urc_labels: (B)\n",
    "            batch_mlm_attentions\n",
    "            batch_urc_attentions\n",
    "\n",
    "        batch가 3\n",
    "        MLM = 3개의 입력데이터 (입력데이터별로 길이가 다름)\n",
    "        URC = 9개의 입력데이터 (context는 길이가 다름, response candidate도 길이가 다름)\n",
    "        \"\"\"\n",
    "        (\n",
    "            batch_corrupt_tokens,\n",
    "            batch_output_tokens,\n",
    "            batch_corrupt_mask_positions,\n",
    "            batch_urc_inputs,\n",
    "            batch_urc_labels,\n",
    "        ) = ([], [], [], [], [])\n",
    "        batch_mlm_attentions, batch_urc_attentions = [], []\n",
    "        # MLM, URC 입력에 대해서 가장 긴 입력 길이를 찾기\n",
    "        corrupt_max_len, urc_max_len = 0, 0\n",
    "        for session in sessions:\n",
    "            (\n",
    "                corrupt_tokens,\n",
    "                output_tokens,\n",
    "                corrupt_mask_positions,\n",
    "                urc_inputs,\n",
    "                urc_labels,\n",
    "            ) = session\n",
    "            if len(corrupt_tokens) > corrupt_max_len:\n",
    "                corrupt_max_len = len(corrupt_tokens)\n",
    "            positive_tokens, random_tokens, context_neg_tokens = urc_inputs\n",
    "            if (\n",
    "                max([len(positive_tokens), len(random_tokens), len(context_neg_tokens)])\n",
    "                > urc_max_len\n",
    "            ):\n",
    "                urc_max_len = max(\n",
    "                    [len(positive_tokens), len(random_tokens), len(context_neg_tokens)]\n",
    "                )\n",
    "\n",
    "        ## padding 토큰을 추가하는 부분\n",
    "        for session in sessions:\n",
    "            (\n",
    "                corrupt_tokens,\n",
    "                output_tokens,\n",
    "                corrupt_mask_positions,\n",
    "                urc_inputs,\n",
    "                urc_labels,\n",
    "            ) = session\n",
    "            \"\"\" mlm 입력 \"\"\"\n",
    "            batch_corrupt_tokens.append(\n",
    "                corrupt_tokens\n",
    "                + [\n",
    "                    self.tokenizer.pad_token_id\n",
    "                    for _ in range(corrupt_max_len - len(corrupt_tokens))\n",
    "                ]\n",
    "            )\n",
    "            batch_mlm_attentions.append(\n",
    "                [1 for _ in range(len(corrupt_tokens))]\n",
    "                + [0 for _ in range(corrupt_max_len - len(corrupt_tokens))]\n",
    "            )\n",
    "\n",
    "            \"\"\" mlm 출력 \"\"\"\n",
    "            batch_output_tokens.append(\n",
    "                output_tokens\n",
    "                + [\n",
    "                    self.tokenizer.pad_token_id\n",
    "                    for _ in range(corrupt_max_len - len(corrupt_tokens))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            \"\"\" mlm 레이블 \"\"\"\n",
    "            batch_corrupt_mask_positions.append(corrupt_mask_positions)\n",
    "\n",
    "            \"\"\" urc 입력 \"\"\"\n",
    "            # positive_tokens, random_tokens, context_neg_tokens = urc_inputs\n",
    "            for urc_input in urc_inputs:\n",
    "                batch_urc_inputs.append(\n",
    "                    urc_input\n",
    "                    + [\n",
    "                        self.tokenizer.pad_token_id\n",
    "                        for _ in range(urc_max_len - len(urc_input))\n",
    "                    ]\n",
    "                )\n",
    "                batch_urc_attentions.append(\n",
    "                    [1 for _ in range(len(urc_input))]\n",
    "                    + [0 for _ in range(urc_max_len - len(urc_input))]\n",
    "                )\n",
    "\n",
    "            \"\"\" urc 레이블 \"\"\"\n",
    "            batch_urc_labels += urc_labels\n",
    "        return (\n",
    "            torch.tensor(batch_corrupt_tokens),\n",
    "            torch.tensor(batch_output_tokens),\n",
    "            batch_corrupt_mask_positions,\n",
    "            torch.tensor(batch_urc_inputs),\n",
    "            torch.tensor(batch_urc_labels),\n",
    "            torch.tensor(batch_mlm_attentions),\n",
    "            torch.tensor(batch_urc_attentions),\n",
    "        )\n",
    "\n",
    "    def _set_logger(self):\n",
    "        logging.basicConfig(\n",
    "            format=\"%(message)s\",\n",
    "            level=logging.DEBUG,\n",
    "        )\n",
    "        logger = logging.getLogger()\n",
    "        return logger\n",
    "\n",
    "    def _construct_sessions(self, data_path: str) -> List[List[str]]:\n",
    "        data = pd.read_csv(data_path, sep=\"\\t\")\n",
    "        cols = data.columns.tolist()\n",
    "        logger.debug(f\"Columns: {cols}\")\n",
    "\n",
    "        # use formal conversational data\n",
    "        data = data[[\"formal\"]]\n",
    "        data[\"group\"] = data[\"formal\"].isnull().cumsum()\n",
    "        n_sessions = data[\"group\"].iat[-1] + 1\n",
    "        logger.debug(f\"Number of groups: {n_sessions}\")\n",
    "\n",
    "        # split data into sessions\n",
    "        sessions: List[List[str]] = []\n",
    "        groups = data.groupby(\"group\", as_index=False, group_keys=False)\n",
    "\n",
    "        for i, group in groups:\n",
    "            session = group.dropna()[\"formal\"].tolist()\n",
    "            sessions += [session]\n",
    "        assert n_sessions == len(sessions)\n",
    "        return sessions\n",
    "\n",
    "    def _construct_short_sessions(self, sessions, ctx_len):\n",
    "        short_sessions = []\n",
    "        for session in sessions:\n",
    "            for i in range(len(session) - ctx_len + 1):\n",
    "                short_sessions.append(session[i : i + ctx_len])\n",
    "        return short_sessions\n",
    "\n",
    "    def _get_utterances(self, sessions):\n",
    "        all_utts = set()\n",
    "        for session in sessions:\n",
    "            for utt in session:\n",
    "                all_utts.add(utt)\n",
    "        return list(all_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co:443 \"HEAD /klue/roberta-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "Columns: ['formal', 'informal', 'android', 'azae', 'chat', 'choding', 'emoticon', 'enfp', 'gentle', 'halbae', 'halmae', 'joongding', 'king', 'naruto', 'seonbi', 'sosim', 'translator']\n",
      "Number of groups: 236\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "post_dataset = PostDataset(data_path=DATA_PATH, ctx_len=4)\n",
    "post_dataloader = DataLoader(\n",
    "    post_dataset, batch_size=2, shuffle=True, collate_fn=post_dataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요. 저는 고양이 6 [MASK] 키워요. <SEP> [MASK]를 6마리 [MASK]요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 [MASK] [MASK] 않아요. <SEP> 가장 나이가 많은 고양이가 [MASK] 돼요?\n",
      "안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> 가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "[8, 13, 17, 36, 37, 50]\n",
      "####\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 고양이가 사람 음식을 너무 많이 먹으면 배탈이 납니다.\n",
      "[CLS] 안녕하세요. 저는 고양이 6마리 키워요. <SEP> 고양이를 6마리나요? 키우는거 안 힘드세요? <SEP> 제가 워낙 고양이를 좋아해서 크게 힘들진 않아요. <SEP> [SEP] 안녕하세요. 저는 고양이 6마리 키워요.\n"
     ]
    }
   ],
   "source": [
    "# Test Dataset\n",
    "(\n",
    "    corrupt_tokens,\n",
    "    output_tokens,\n",
    "    corrupt_mask_positions,\n",
    "    urc_inputs,\n",
    "    urc_labels,\n",
    ") = post_dataset[0]\n",
    "print(post_dataset.tokenizer.decode(corrupt_tokens))\n",
    "print(post_dataset.tokenizer.decode(output_tokens))\n",
    "print(corrupt_mask_positions)\n",
    "print(\"####\")\n",
    "print(post_dataset.tokenizer.decode(urc_inputs[0]))\n",
    "print(post_dataset.tokenizer.decode(urc_inputs[1]))\n",
    "print(post_dataset.tokenizer.decode(urc_inputs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataLoader\n",
    "(\n",
    "    batch_corrupt_tokens,\n",
    "    batch_output_tokens,\n",
    "    batch_corrupt_mask_positions,\n",
    "    batch_urc_inputs,\n",
    "    batch_urc_labels,\n",
    "    batch_mlm_attentions,\n",
    "    batch_urc_attentions,\n",
    ") = next(iter(post_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] csr가 뭔지는 모르겠지만, 도어 대시라는 회사는 알고 있습니다. 큰 회사였잖아요. <SEP> csr는 고객 서비스 담당자의 약자입니다. 해당 부서가 축소되면서 저도 잘렸습니다. <SEP> 그러면 지금은 뭘 하는 중이세요? <SEP> [SEP] 경쟁 회사에 취직하고 나서는, 일 뿐만 아니라 취미활동도 하고 있습니다. 퇴근하고 나면 근처에 있는 종합체육시설에서 수영을 해요.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_dataset.tokenizer.decode(batch_urc_inputs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
